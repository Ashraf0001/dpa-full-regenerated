{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Processing Accelerator (DPA)","text":""},{"location":"#what-is-dpa","title":"What is DPA?","text":"<p>DPA (Data Processing Accelerator) is a high-performance data processing tool that combines the speed of Rust with the power of Polars to provide lightning-fast data operations. Whether you're working with CSV files, Parquet datasets, or JSON data, DPA offers both command-line and Python interfaces for seamless data manipulation.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#performance","title":"Performance","text":"<ul> <li>Rust Backend: Compiled to native code for maximum speed</li> <li>Polars Integration: Leverages Polars' optimized data processing</li> <li>Lazy Evaluation: Efficient memory usage with lazy computation</li> <li>Parallel Processing: Automatic parallelization where possible</li> </ul>"},{"location":"#data-operations","title":"Data Operations","text":"<ul> <li>Enhanced Profiling: Comprehensive data analysis with statistical summaries</li> <li>Data Validation: Schema enforcement and quality checks</li> <li>Smart Sampling: Random, stratified, and systematic sampling</li> <li>SQL-like Filtering: Powerful filtering with SQL expressions</li> <li>Advanced Aggregations: Group-by operations with multiple aggregations</li> <li>Data Joins: Inner and left joins with optimized performance</li> </ul>"},{"location":"#developer-experience","title":"Developer Experience","text":"<ul> <li>Dual Interface: Both CLI and Python API</li> <li>Multiple Formats: CSV, Parquet, JSON, JSONL support</li> <li>Error Handling: Comprehensive error messages and validation</li> <li>Progress Indicators: Real-time progress for long operations</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Enhanced data profiling\ndpa profile data/transactions.csv --detailed\n\n# Data validation with custom rules\ndpa validate data/transactions.csv --schema schema.json --rules validation_rules.json\n\n# Stratified sampling for ML\ndpa sample data/transactions.csv -o sample.csv --method stratified --stratify country --size 1000\n\n# Train/test split\ndpa split data/transactions.csv --train train.csv --test test.csv --test-size 0.2 --stratify country\n</code></pre> <pre><code>import dpa_core\n\n# Enhanced profiling\nprofile = dpa_core.profile_py(\"data/transactions.csv\")\nprint(f\"Memory usage: {profile['memory_mb']} MB\")\nprint(f\"Null percentage: {profile['null_percentage']}%\")\n\n# Data validation\ndpa_core.validate_py(\"data/transactions.csv\", \"schema.json\", \"rules.json\")\n</code></pre>"},{"location":"#performance-benchmarks","title":"Performance Benchmarks","text":"<p>DPA consistently outperforms traditional data processing tools:</p> Operation DPA pandas dask CSV Read (1GB) 2.1s 8.5s 4.2s Filter (1M rows) 0.3s 1.2s 0.8s Group By 0.8s 3.1s 1.9s Memory Usage 45MB 180MB 120MB"},{"location":"#installation","title":"Installation","text":""},{"location":"#quick-install","title":"Quick Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/Ashraf0001/dpa-full-regenerated.git\ncd dpa-full-regenerated\n\n# Build Rust binary\ncargo build --release\n\n# Install Python bindings\npip install maturin\nmaturin develop\n\n# Install Python CLI\ncd python &amp;&amp; pip install .\n</code></pre> <p>For detailed installation instructions, see Installation Guide.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started: Quick setup and first steps</li> <li>User Guide: Comprehensive overview and usage</li> <li>API Reference: Complete CLI and Python API documentation</li> <li>Examples: Real-world usage examples</li> <li>Features: Detailed feature documentation</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>   **Made with \u2764\ufe0f by the DPA Team**  [GitHub](https://github.com/Ashraf0001/dpa-full-regenerated) \u2022 [Issues](https://github.com/Ashraf0001/dpa-full-regenerated/issues) \u2022 [Discussions](https://github.com/Ashraf0001/dpa-full-regenerated/discussions)"},{"location":"api/cli-commands/","title":"CLI Commands Reference","text":"<p>Complete reference for all DPA command-line interface commands.</p>"},{"location":"api/cli-commands/#overview","title":"Overview","text":"<p>DPA provides a comprehensive CLI with commands for data processing, profiling, validation, and more. All commands follow a consistent pattern:</p> <pre><code>./target/release/dpa &lt;command&gt; [options] &lt;arguments&gt;\n</code></pre>"},{"location":"api/cli-commands/#global-options","title":"Global Options","text":"<p>All commands support these global options:</p> <pre><code>--help, -h          Show help for the command\n--version, -V       Show version information\n--config &lt;file&gt;     Use specified configuration file\n--log-level &lt;level&gt; Set log level (debug, info, warn, error)\n--quiet, -q         Suppress output\n--verbose, -v       Increase verbosity\n</code></pre>"},{"location":"api/cli-commands/#commands","title":"Commands","text":""},{"location":"api/cli-commands/#schema-print-schema-information","title":"<code>schema</code> - Print Schema Information","text":"<p>Display the schema and structure of a data file.</p> <pre><code>./target/release/dpa schema &lt;input&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path (CSV, Parquet, or JSON)</p> <p>Options: - <code>--format &lt;format&gt;</code> - Output format (table, json, yaml) - <code>--detailed</code> - Show detailed column information</p> <p>Examples: <pre><code># Basic schema\n./target/release/dpa schema data.csv\n\n# Detailed schema in JSON format\n./target/release/dpa schema data.csv --format json --detailed\n\n# Schema for Parquet file\n./target/release/dpa schema data.parquet\n</code></pre></p> <p>Output: <pre><code>Schema for data.csv:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Column  \u2502 Type    \u2502 Nullable\u2502 Count   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id      \u2502 Int64   \u2502 false   \u2502 1000    \u2502\n\u2502 name    \u2502 String  \u2502 false   \u2502 1000    \u2502\n\u2502 age     \u2502 Int64   \u2502 true    \u2502 950     \u2502\n\u2502 salary  \u2502 Float64 \u2502 true    \u2502 980     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api/cli-commands/#head-preview-data","title":"<code>head</code> - Preview Data","text":"<p>Display the first N rows of a data file.</p> <pre><code>./target/release/dpa head [options] &lt;input&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path</p> <p>Options: - <code>-n, --n &lt;number&gt;</code> - Number of rows to display (default: 10) - <code>--format &lt;format&gt;</code> - Output format (table, csv, json)</p> <p>Examples: <pre><code># Show first 10 rows\n./target/release/dpa head data.csv\n\n# Show first 5 rows\n./target/release/dpa head data.csv -n 5\n\n# Show in JSON format\n./target/release/dpa head data.csv --format json\n</code></pre></p>"},{"location":"api/cli-commands/#profile-data-profiling","title":"<code>profile</code> - Data Profiling","text":"<p>Generate comprehensive data profiling reports.</p> <pre><code>./target/release/dpa profile [options] &lt;input&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path</p> <p>Options: - <code>--detailed</code> - Include detailed statistics - <code>--sample &lt;size&gt;</code> - Sample size for large datasets - <code>--output &lt;file&gt;</code> - Save report to file - <code>--format &lt;format&gt;</code> - Output format (table, json, html)</p> <p>Examples: <pre><code># Basic profiling\n./target/release/dpa profile data.csv\n\n# Detailed profiling with sampling\n./target/release/dpa profile data.csv --detailed --sample 10000\n\n# Save report to file\n./target/release/dpa profile data.csv --output report.json --format json\n</code></pre></p> <p>Output: <pre><code>\ud83d\udcca Data Profile Report\n=====================\n\n\ud83d\udcc1 File: data.csv\n\ud83d\udccf Shape: 10,000 rows \u00d7 5 columns\n\ud83d\udcbe Size: 2.3 MB\n\u23f1\ufe0f  Processing time: 0.8s\n\n\ud83d\udcc8 Column Statistics:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Column  \u2502 Type    \u2502 Null %  \u2502 Unique  \u2502 Min     \u2502 Max     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id      \u2502 Int64   \u2502 0.0%    \u2502 10,000  \u2502 1       \u2502 10,000  \u2502\n\u2502 name    \u2502 String  \u2502 0.0%    \u2502 9,850   \u2502 -       \u2502 -       \u2502\n\u2502 age     \u2502 Int64   \u2502 5.0%    \u2502 45      \u2502 18      \u2502 85      \u2502\n\u2502 salary  \u2502 Float64 \u2502 2.0%    \u2502 8,950   \u2502 25,000  \u2502 150,000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"api/cli-commands/#validate-data-validation","title":"<code>validate</code> - Data Validation","text":"<p>Validate data against schemas and rules.</p> <pre><code>./target/release/dpa validate [options] &lt;input&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path</p> <p>Options: - <code>--schema &lt;file&gt;</code> - Schema file for validation - <code>--rules &lt;file&gt;</code> - Custom validation rules file - <code>--output &lt;file&gt;</code> - Save validation report - <code>--max-errors &lt;number&gt;</code> - Maximum errors to report (default: 1000) - <code>--strict</code> - Fail on first error</p> <p>Examples: <pre><code># Basic validation\n./target/release/dpa validate data.csv\n\n# Schema validation\n./target/release/dpa validate data.csv --schema schema.json\n\n# Custom rules validation\n./target/release/dpa validate data.csv --rules rules.json\n\n# Strict validation\n./target/release/dpa validate data.csv --schema schema.json --strict\n</code></pre></p> <p>Output: <pre><code>\u2705 Data Validation Report\n========================\n\n\ud83d\udcc1 File: data.csv\n\ud83d\udd0d Validation: PASSED\n\u23f1\ufe0f  Processing time: 0.3s\n\n\ud83d\udcca Summary:\n- \u2705 Schema validation: PASSED\n- \u2705 Data type validation: PASSED\n- \u2705 Range validation: PASSED\n- \u26a0\ufe0f  Custom rules: 2 warnings\n\n\u26a0\ufe0f  Warnings:\n- Column 'age': 5 values outside expected range (18-65)\n- Column 'salary': 3 negative values detected\n</code></pre></p>"},{"location":"api/cli-commands/#sample-data-sampling","title":"<code>sample</code> - Data Sampling","text":"<p>Create samples from datasets using various methods.</p> <pre><code>./target/release/dpa sample [options] &lt;input&gt; &lt;output&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>output</code> - Output file path</p> <p>Options: - <code>--method &lt;method&gt;</code> - Sampling method (random, stratified, head, tail) - <code>--size &lt;number&gt;</code> - Sample size - <code>--stratify &lt;column&gt;</code> - Column for stratified sampling - <code>--seed &lt;number&gt;</code> - Random seed for reproducibility - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Random sampling\n./target/release/dpa sample data.csv sample.csv --method random --size 1000\n\n# Stratified sampling\n./target/release/dpa sample data.csv sample.csv --method stratified --stratify category --size 500\n\n# Head sampling\n./target/release/dpa sample data.csv sample.csv --method head --size 100\n\n# With seed for reproducibility\n./target/release/dpa sample data.csv sample.csv --method random --size 1000 --seed 42\n</code></pre></p>"},{"location":"api/cli-commands/#split-traintest-split","title":"<code>split</code> - Train/Test Split","text":"<p>Split datasets into training and test sets.</p> <pre><code>./target/release/dpa split [options] &lt;input&gt; &lt;train_output&gt; &lt;test_output&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>train_output</code> - Training set output file - <code>test_output</code> - Test set output file</p> <p>Options: - <code>--test-size &lt;fraction&gt;</code> - Test set fraction (default: 0.2) - <code>--stratify &lt;column&gt;</code> - Column for stratified splitting - <code>--seed &lt;number&gt;</code> - Random seed for reproducibility - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Basic split\n./target/release/dpa split data.csv train.csv test.csv\n\n# Custom test size\n./target/release/dpa split data.csv train.csv test.csv --test-size 0.3\n\n# Stratified split\n./target/release/dpa split data.csv train.csv test.csv --stratify category\n\n# With seed\n./target/release/dpa split data.csv train.csv test.csv --seed 42\n</code></pre></p>"},{"location":"api/cli-commands/#filter-data-filtering","title":"<code>filter</code> - Data Filtering","text":"<p>Filter data using SQL-like expressions.</p> <pre><code>./target/release/dpa filter [options] &lt;input&gt; &lt;expression&gt; [output]\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>expression</code> - Filter expression (SQL-like) - <code>output</code> - Output file path (optional)</p> <p>Options: - <code>--select &lt;columns&gt;</code> - Select specific columns - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Basic filtering\n./target/release/dpa filter data.csv \"age &gt; 30\" filtered.csv\n\n# Filter with column selection\n./target/release/dpa filter data.csv \"salary &gt; 50000\" --select \"name,age,salary\" filtered.csv\n\n# Complex expression\n./target/release/dpa filter data.csv \"age &gt; 25 AND salary &gt; 40000 AND city = 'New York'\" filtered.csv\n\n# Output to different format\n./target/release/dpa filter data.csv \"age &gt; 30\" filtered.parquet --format parquet\n</code></pre></p>"},{"location":"api/cli-commands/#select-column-selection","title":"<code>select</code> - Column Selection","text":"<p>Select specific columns from datasets.</p> <pre><code>./target/release/dpa select [options] &lt;input&gt; &lt;columns&gt; [output]\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>columns</code> - Comma-separated list of columns - <code>output</code> - Output file path (optional)</p> <p>Options: - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Select specific columns\n./target/release/dpa select data.csv \"name,age,salary\" selected.csv\n\n# Select all columns except some\n./target/release/dpa select data.csv \"!id,!timestamp\" selected.csv\n\n# Output to different format\n./target/release/dpa select data.csv \"name,age\" selected.parquet --format parquet\n</code></pre></p>"},{"location":"api/cli-commands/#convert-format-conversion","title":"<code>convert</code> - Format Conversion","text":"<p>Convert between different file formats.</p> <pre><code>./target/release/dpa convert &lt;input&gt; &lt;output&gt;\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>output</code> - Output file path</p> <p>Examples: <pre><code># CSV to Parquet\n./target/release/dpa convert data.csv data.parquet\n\n# Parquet to CSV\n./target/release/dpa convert data.parquet data.csv\n\n# CSV to JSON\n./target/release/dpa convert data.csv data.json\n\n# JSON to Parquet\n./target/release/dpa convert data.json data.parquet\n</code></pre></p>"},{"location":"api/cli-commands/#agg-aggregations","title":"<code>agg</code> - Aggregations","text":"<p>Perform groupby aggregations on data.</p> <pre><code>./target/release/dpa agg [options] &lt;input&gt; &lt;group_by&gt; &lt;aggregations&gt; [output]\n</code></pre> <p>Arguments: - <code>input</code> - Input file path - <code>group_by</code> - Column(s) to group by - <code>aggregations</code> - Aggregation expressions - <code>output</code> - Output file path (optional)</p> <p>Options: - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Simple aggregation\n./target/release/dpa agg data.csv \"category\" \"count(), avg(salary), sum(revenue)\" agg.csv\n\n# Multiple group columns\n./target/release/dpa agg data.csv \"category,region\" \"count(), avg(age)\" agg.csv\n\n# Complex aggregations\n./target/release/dpa agg data.csv \"department\" \"count() as emp_count, avg(salary) as avg_salary, max(age) as max_age\" agg.csv\n</code></pre></p>"},{"location":"api/cli-commands/#join-data-joins","title":"<code>join</code> - Data Joins","text":"<p>Join multiple datasets.</p> <pre><code>./target/release/dpa join [options] &lt;left&gt; &lt;right&gt; &lt;on&gt; [output]\n</code></pre> <p>Arguments: - <code>left</code> - Left dataset file path - <code>right</code> - Right dataset file path - <code>on</code> - Join condition - <code>output</code> - Output file path (optional)</p> <p>Options: - <code>--how &lt;type&gt;</code> - Join type (inner, left, right, outer) - <code>--format &lt;format&gt;</code> - Output format (csv, parquet, json)</p> <p>Examples: <pre><code># Inner join\n./target/release/dpa join users.csv orders.csv \"users.id = orders.user_id\" joined.csv\n\n# Left join\n./target/release/dpa join users.csv orders.csv \"users.id = orders.user_id\" --how left joined.csv\n\n# Multiple join conditions\n./target/release/dpa join data1.csv data2.csv \"data1.id = data2.id AND data1.date = data2.date\" joined.csv\n</code></pre></p>"},{"location":"api/cli-commands/#command-aliases","title":"Command Aliases","text":"<p>For convenience, DPA provides short aliases for common commands:</p> Full Command Alias Description <code>filter</code> <code>f</code> Filter data <code>select</code> <code>s</code> Select columns <code>convert</code> <code>c</code> Convert formats <code>profile</code> <code>p</code> Profile data <code>validate</code> <code>v</code> Validate data <code>agg</code> <code>a</code> Aggregations <code>join</code> <code>j</code> Join datasets <p>Examples: <pre><code># Using aliases\n./target/release/dpa f data.csv \"age &gt; 30\" filtered.csv\n./target/release/dpa s data.csv \"name,age\" selected.csv\n./target/release/dpa p data.csv\n./target/release/dpa v data.csv\n</code></pre></p>"},{"location":"api/cli-commands/#error-handling","title":"Error Handling","text":""},{"location":"api/cli-commands/#common-error-messages","title":"Common Error Messages","text":"<p>File Not Found: <pre><code>Error: File 'data.csv' not found\n</code></pre></p> <p>Invalid Expression: <pre><code>Error: Invalid filter expression 'age &gt;'\n</code></pre></p> <p>Schema Mismatch: <pre><code>Error: Schema validation failed for column 'age'\n</code></pre></p> <p>Memory Error: <pre><code>Error: Insufficient memory for operation\n</code></pre></p>"},{"location":"api/cli-commands/#debugging","title":"Debugging","text":"<p>Use the <code>--verbose</code> flag for detailed error information:</p> <pre><code>./target/release/dpa profile data.csv --verbose\n</code></pre>"},{"location":"api/cli-commands/#performance-tips","title":"Performance Tips","text":""},{"location":"api/cli-commands/#for-large-datasets","title":"For Large Datasets","text":"<pre><code># Use sampling for profiling\n./target/release/dpa profile large_data.csv --sample 10000\n\n# Use Parquet format for better performance\n./target/release/dpa convert large_data.csv large_data.parquet\n./target/release/dpa profile large_data.parquet\n\n# Use chunked processing\nexport DPA_CHUNK_SIZE=50000\n./target/release/dpa profile large_data.csv\n</code></pre>"},{"location":"api/cli-commands/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Reduce memory usage\nexport DPA_MAX_MEMORY=4GB\n./target/release/dpa profile data.csv\n\n# Use streaming for very large files\nexport DPA_USE_STREAMING=true\n./target/release/dpa convert large_data.csv large_data.parquet\n</code></pre>"},{"location":"api/cli-commands/#configuration","title":"Configuration","text":"<p>Commands can be configured using:</p> <ul> <li>Environment variables: <code>export DPA_LOG_LEVEL=debug</code></li> <li>Configuration file: <code>--config config.toml</code></li> <li>Command-line options: <code>--log-level debug</code></li> </ul> <p>See the Configuration Guide for details.</p>"},{"location":"api/cli-commands/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Python API: Learn about the Python API Reference</li> <li>\ud83c\udfaf Examples: See practical examples in the Examples section</li> <li>\ud83d\udd27 Configuration: Understand Configuration Options</li> <li>\ud83d\ude80 Advanced Usage: Explore Best Practices</li> </ul> <p>Ready to use the CLI? Start with the Quick Start Guide to get familiar with the commands.</p>"},{"location":"api/python-api/","title":"Python API Reference","text":"<p>Complete reference for the DPA Python API with PyO3 bindings.</p>"},{"location":"api/python-api/#overview","title":"Overview","text":"<p>DPA provides a comprehensive Python API through PyO3 bindings, offering high-performance data processing functions that can be seamlessly integrated into Python workflows.</p>"},{"location":"api/python-api/#installation","title":"Installation","text":""},{"location":"api/python-api/#from-source","title":"From Source","text":"<pre><code># Install maturin\npip install maturin\n\n# Build and install Python bindings\nmaturin develop\n</code></pre>"},{"location":"api/python-api/#import","title":"Import","text":"<pre><code>import dpa_core\n</code></pre>"},{"location":"api/python-api/#core-functions","title":"Core Functions","text":""},{"location":"api/python-api/#profile_py-data-profiling","title":"<code>profile_py()</code> - Data Profiling","text":"<p>Generate comprehensive data profiling reports.</p> <pre><code>dpa_core.profile_py(input: str) -&gt; dict\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file (CSV, Parquet, or JSON)</p> <p>Returns: - <code>dict</code>: Dictionary containing profiling statistics</p> <p>Example: <pre><code>import dpa_core\n\n# Basic profiling\nstats = dpa_core.profile_py(\"data.csv\")\nprint(stats)\n\n# Access specific statistics\nprint(f\"Total rows: {stats['total_rows']}\")\nprint(f\"Total columns: {stats['total_columns']}\")\nprint(f\"Memory usage: {stats['memory_usage']}\")\n</code></pre></p> <p>Return Value Structure: <pre><code>{\n    'total_rows': 10000,\n    'total_columns': 5,\n    'memory_usage': '2.3 MB',\n    'file_size': '1.8 MB',\n    'processing_time': '0.8s',\n    'columns': {\n        'id': {\n            'type': 'Int64',\n            'null_count': 0,\n            'null_percentage': 0.0,\n            'unique_count': 10000,\n            'min': 1,\n            'max': 10000,\n            'mean': 5000.5,\n            'std': 2886.9\n        },\n        'name': {\n            'type': 'String',\n            'null_count': 0,\n            'null_percentage': 0.0,\n            'unique_count': 9850,\n            'avg_length': 12.3\n        }\n        # ... more columns\n    }\n}\n</code></pre></p>"},{"location":"api/python-api/#validate_py-data-validation","title":"<code>validate_py()</code> - Data Validation","text":"<p>Validate data against schemas and rules.</p> <pre><code>dpa_core.validate_py(\n    input: str,\n    schema: Optional[str] = None,\n    rules: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>schema</code> (str, optional): Path to schema file - <code>rules</code> (str, optional): Path to validation rules file</p> <p>Raises: - <code>RuntimeError</code>: If validation fails</p> <p>Example: <pre><code>import dpa_core\n\n# Basic validation\ntry:\n    dpa_core.validate_py(\"data.csv\")\n    print(\"\u2705 Validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Validation failed: {e}\")\n\n# Schema validation\ntry:\n    dpa_core.validate_py(\"data.csv\", schema=\"schema.json\")\n    print(\"\u2705 Schema validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Schema validation failed: {e}\")\n\n# Custom rules validation\ntry:\n    dpa_core.validate_py(\"data.csv\", rules=\"rules.json\")\n    print(\"\u2705 Rules validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Rules validation failed: {e}\")\n</code></pre></p>"},{"location":"api/python-api/#sample_py-data-sampling","title":"<code>sample_py()</code> - Data Sampling","text":"<p>Create samples from datasets using various methods.</p> <pre><code>dpa_core.sample_py(\n    input: str,\n    output: str,\n    size: Optional[int] = None,\n    method: Optional[str] = None,\n    stratify: Optional[str] = None,\n    seed: Optional[int] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>output</code> (str): Path to output file - <code>size</code> (int, optional): Sample size (default: 1000) - <code>method</code> (str, optional): Sampling method - \"random\", \"stratified\", \"head\", \"tail\" (default: \"random\") - <code>stratify</code> (str, optional): Column for stratified sampling - <code>seed</code> (int, optional): Random seed for reproducibility</p> <p>Example: <pre><code>import dpa_core\n\n# Random sampling\ndpa_core.sample_py(\"data.csv\", \"sample.csv\", size=1000, method=\"random\")\n\n# Stratified sampling\ndpa_core.sample_py(\"data.csv\", \"sample.csv\", size=500, method=\"stratified\", stratify=\"category\")\n\n# Head sampling\ndpa_core.sample_py(\"data.csv\", \"sample.csv\", size=100, method=\"head\")\n\n# With seed for reproducibility\ndpa_core.sample_py(\"data.csv\", \"sample.csv\", size=1000, method=\"random\", seed=42)\n</code></pre></p>"},{"location":"api/python-api/#split_py-traintest-split","title":"<code>split_py()</code> - Train/Test Split","text":"<p>Split datasets into training and test sets.</p> <pre><code>dpa_core.split_py(\n    input: str,\n    train_output: str,\n    test_output: str,\n    test_size: Optional[float] = None,\n    stratify: Optional[str] = None,\n    seed: Optional[int] = None\n) -&gt; None\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>train_output</code> (str): Path to training set output file - <code>test_output</code> (str): Path to test set output file - <code>test_size</code> (float, optional): Test set fraction (default: 0.2) - <code>stratify</code> (str, optional): Column for stratified splitting - <code>seed</code> (int, optional): Random seed for reproducibility</p> <p>Example: <pre><code>import dpa_core\n\n# Basic split\ndpa_core.split_py(\"data.csv\", \"train.csv\", \"test.csv\")\n\n# Custom test size\ndpa_core.split_py(\"data.csv\", \"train.csv\", \"test.csv\", test_size=0.3)\n\n# Stratified split\ndpa_core.split_py(\"data.csv\", \"train.csv\", \"test.csv\", stratify=\"category\")\n\n# With seed\ndpa_core.split_py(\"data.csv\", \"train.csv\", \"test.csv\", seed=42)\n</code></pre></p>"},{"location":"api/python-api/#filter_py-data-filtering","title":"<code>filter_py()</code> - Data Filtering","text":"<p>Filter data using SQL-like expressions.</p> <pre><code>dpa_core.filter_py(\n    input: str,\n    where_expr: str,\n    select: Optional[List[str]] = None,\n    output: Optional[str] = None\n) -&gt; str\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>where_expr</code> (str): Filter expression (SQL-like) - <code>select</code> (List[str], optional): List of columns to select - <code>output</code> (str, optional): Path to output file</p> <p>Returns: - <code>str</code>: Path to output file</p> <p>Example: <pre><code>import dpa_core\n\n# Basic filtering\noutput = dpa_core.filter_py(\"data.csv\", \"age &gt; 30\", output=\"filtered.csv\")\n\n# Filter with column selection\noutput = dpa_core.filter_py(\n    \"data.csv\", \n    \"salary &gt; 50000\", \n    select=[\"name\", \"age\", \"salary\"], \n    output=\"filtered.csv\"\n)\n\n# Complex expression\noutput = dpa_core.filter_py(\n    \"data.csv\", \n    \"age &gt; 25 AND salary &gt; 40000 AND city = 'New York'\", \n    output=\"filtered.csv\"\n)\n</code></pre></p>"},{"location":"api/python-api/#select_py-column-selection","title":"<code>select_py()</code> - Column Selection","text":"<p>Select specific columns from datasets.</p> <pre><code>dpa_core.select_py(\n    input: str,\n    columns: List[str],\n    output: Optional[str] = None\n) -&gt; str\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>columns</code> (List[str]): List of column names to select - <code>output</code> (str, optional): Path to output file</p> <p>Returns: - <code>str</code>: Path to output file</p> <p>Example: <pre><code>import dpa_core\n\n# Select specific columns\noutput = dpa_core.select_py(\"data.csv\", [\"name\", \"age\", \"salary\"], output=\"selected.csv\")\n\n# Select all columns except some (using negative selection)\noutput = dpa_core.select_py(\"data.csv\", [\"!id\", \"!timestamp\"], output=\"selected.csv\")\n</code></pre></p>"},{"location":"api/python-api/#convert_py-format-conversion","title":"<code>convert_py()</code> - Format Conversion","text":"<p>Convert between different file formats.</p> <pre><code>dpa_core.convert_py(input: str, output: str) -&gt; str\n</code></pre> <p>Parameters: - <code>input</code> (str): Path to input file - <code>output</code> (str): Path to output file</p> <p>Returns: - <code>str</code>: Path to output file</p> <p>Example: <pre><code>import dpa_core\n\n# CSV to Parquet\noutput = dpa_core.convert_py(\"data.csv\", \"data.parquet\")\n\n# Parquet to CSV\noutput = dpa_core.convert_py(\"data.parquet\", \"data.csv\")\n\n# CSV to JSON\noutput = dpa_core.convert_py(\"data.csv\", \"data.json\")\n\n# JSON to Parquet\noutput = dpa_core.convert_py(\"data.json\", \"data.parquet\")\n</code></pre></p>"},{"location":"api/python-api/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/python-api/#error-handling","title":"Error Handling","text":"<pre><code>import dpa_core\n\ndef safe_validate(file_path: str, schema_path: str = None) -&gt; bool:\n    \"\"\"Safely validate a file with optional schema.\"\"\"\n    try:\n        if schema_path:\n            dpa_core.validate_py(file_path, schema=schema_path)\n        else:\n            dpa_core.validate_py(file_path)\n        return True\n    except RuntimeError as e:\n        print(f\"Validation failed: {e}\")\n        return False\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return False\n\n# Usage\nif safe_validate(\"data.csv\", \"schema.json\"):\n    print(\"Data is valid!\")\nelse:\n    print(\"Data validation failed!\")\n</code></pre>"},{"location":"api/python-api/#batch-processing","title":"Batch Processing","text":"<pre><code>import dpa_core\nimport os\nfrom pathlib import Path\n\ndef process_directory(input_dir: str, output_dir: str):\n    \"\"\"Process all CSV files in a directory.\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(exist_ok=True)\n\n    for csv_file in input_path.glob(\"*.csv\"):\n        # Profile each file\n        stats = dpa_core.profile_py(str(csv_file))\n        print(f\"Processed {csv_file.name}: {stats['total_rows']} rows\")\n\n        # Sample each file\n        sample_file = output_path / f\"sample_{csv_file.name}\"\n        dpa_core.sample_py(str(csv_file), str(sample_file), size=1000)\n\n        # Convert to Parquet\n        parquet_file = output_path / f\"{csv_file.stem}.parquet\"\n        dpa_core.convert_py(str(csv_file), str(parquet_file))\n\n# Usage\nprocess_directory(\"raw_data\", \"processed_data\")\n</code></pre>"},{"location":"api/python-api/#data-quality-pipeline","title":"Data Quality Pipeline","text":"<pre><code>import dpa_core\nimport pandas as pd\n\ndef data_quality_pipeline(input_file: str, output_dir: str):\n    \"\"\"Complete data quality assessment pipeline.\"\"\"\n\n    # 1. Profile the data\n    print(\"\ud83d\udcca Profiling data...\")\n    stats = dpa_core.profile_py(input_file)\n\n    # 2. Check for quality issues\n    quality_issues = []\n    for col, col_stats in stats['columns'].items():\n        if col_stats['null_percentage'] &gt; 0.1:\n            quality_issues.append(f\"High null percentage in {col}: {col_stats['null_percentage']}%\")\n\n    # 3. Validate the data\n    print(\"\ud83d\udd0d Validating data...\")\n    try:\n        dpa_core.validate_py(input_file)\n        print(\"\u2705 Data validation passed!\")\n    except RuntimeError as e:\n        quality_issues.append(f\"Validation failed: {e}\")\n\n    # 4. Create clean sample\n    print(\"\ud83c\udfaf Creating sample...\")\n    sample_file = f\"{output_dir}/clean_sample.csv\"\n    dpa_core.sample_py(input_file, sample_file, size=1000, method=\"random\")\n\n    # 5. Create train/test split\n    print(\"\u2702\ufe0f  Creating train/test split...\")\n    train_file = f\"{output_dir}/train.csv\"\n    test_file = f\"{output_dir}/test.csv\"\n    dpa_core.split_py(sample_file, train_file, test_file, test_size=0.2)\n\n    # 6. Generate report\n    report = {\n        'input_file': input_file,\n        'total_rows': stats['total_rows'],\n        'total_columns': stats['total_columns'],\n        'quality_issues': quality_issues,\n        'sample_file': sample_file,\n        'train_file': train_file,\n        'test_file': test_file\n    }\n\n    return report\n\n# Usage\nreport = data_quality_pipeline(\"data.csv\", \"output\")\nprint(\"Pipeline completed!\")\nprint(f\"Quality issues found: {len(report['quality_issues'])}\")\n</code></pre>"},{"location":"api/python-api/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import dpa_core\nimport time\nimport psutil\n\ndef monitor_performance(func, *args, **kwargs):\n    \"\"\"Monitor performance of DPA operations.\"\"\"\n    process = psutil.Process()\n    start_memory = process.memory_info().rss / 1024 / 1024  # MB\n    start_time = time.time()\n\n    try:\n        result = func(*args, **kwargs)\n        success = True\n    except Exception as e:\n        result = e\n        success = False\n\n    end_time = time.time()\n    end_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n    performance = {\n        'success': success,\n        'execution_time': end_time - start_time,\n        'memory_used': end_memory - start_memory,\n        'result': result\n    }\n\n    return performance\n\n# Usage\nperf = monitor_performance(dpa_core.profile_py, \"large_data.csv\")\nprint(f\"Execution time: {perf['execution_time']:.2f}s\")\nprint(f\"Memory used: {perf['memory_used']:.2f}MB\")\n</code></pre>"},{"location":"api/python-api/#integration-examples","title":"Integration Examples","text":""},{"location":"api/python-api/#with-pandas","title":"With Pandas","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Use DPA for heavy operations, Pandas for analysis\ndef hybrid_processing(data_file: str):\n    # Use DPA for profiling\n    stats = dpa_core.profile_py(data_file)\n\n    # Use DPA for sampling large files\n    dpa_core.sample_py(data_file, \"sample.csv\", size=10000)\n\n    # Use Pandas for analysis\n    df = pd.read_csv(\"sample.csv\")\n\n    # Continue with Pandas operations\n    summary = df.describe()\n    correlations = df.corr()\n\n    return stats, summary, correlations\n</code></pre>"},{"location":"api/python-api/#with-numpy","title":"With NumPy","text":"<pre><code>import dpa_core\nimport numpy as np\nimport pandas as pd\n\ndef statistical_analysis(data_file: str):\n    # Get basic stats from DPA\n    stats = dpa_core.profile_py(data_file)\n\n    # Load data for detailed analysis\n    df = pd.read_csv(data_file)\n\n    # Use NumPy for advanced statistics\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n\n    advanced_stats = {}\n    for col in numeric_cols:\n        values = df[col].dropna()\n        advanced_stats[col] = {\n            'skewness': float(np.corrcoef(values, np.arange(len(values)))[0, 1]),\n            'kurtosis': float(np.corrcoef(values, np.arange(len(values))**2)[0, 1]),\n            'percentiles': np.percentile(values, [25, 50, 75]).tolist()\n        }\n\n    return stats, advanced_stats\n</code></pre>"},{"location":"api/python-api/#with-scikit-learn","title":"With Scikit-learn","text":"<pre><code>import dpa_core\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef ml_pipeline(data_file: str):\n    # Use DPA for data preparation\n    dpa_core.sample_py(data_file, \"sample.csv\", size=10000, method=\"stratified\", stratify=\"target\")\n    dpa_core.split_py(\"sample.csv\", \"train.csv\", \"test.csv\", test_size=0.2, stratify=\"target\")\n\n    # Load data for ML\n    train_df = pd.read_csv(\"train.csv\")\n    test_df = pd.read_csv(\"test.csv\")\n\n    # Prepare features\n    X_train = train_df.drop('target', axis=1)\n    y_train = train_df['target']\n    X_test = test_df.drop('target', axis=1)\n    y_test = test_df['target']\n\n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    score = model.score(X_test, y_test)\n\n    return model, score\n</code></pre>"},{"location":"api/python-api/#best-practices","title":"Best Practices","text":""},{"location":"api/python-api/#1-error-handling","title":"1. Error Handling","text":"<pre><code>def robust_dpa_operation(operation_func, *args, **kwargs):\n    \"\"\"Wrapper for robust DPA operations.\"\"\"\n    try:\n        return operation_func(*args, **kwargs)\n    except RuntimeError as e:\n        print(f\"DPA operation failed: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n</code></pre>"},{"location":"api/python-api/#2-memory-management","title":"2. Memory Management","text":"<pre><code>import gc\n\ndef memory_efficient_processing(data_file: str):\n    \"\"\"Memory-efficient data processing.\"\"\"\n    # Process in chunks\n    dpa_core.sample_py(data_file, \"sample.csv\", size=5000)\n\n    # Force garbage collection\n    gc.collect()\n\n    # Continue processing\n    stats = dpa_core.profile_py(\"sample.csv\")\n\n    return stats\n</code></pre>"},{"location":"api/python-api/#3-configuration","title":"3. Configuration","text":"<pre><code>import os\n\ndef configure_dpa():\n    \"\"\"Configure DPA for optimal performance.\"\"\"\n    os.environ['DPA_MAX_MEMORY'] = '4GB'\n    os.environ['DPA_NUM_THREADS'] = '4'\n    os.environ['DPA_LOG_LEVEL'] = 'info'\n</code></pre>"},{"location":"api/python-api/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/python-api/#common-issues","title":"Common Issues","text":"<p>1. Import Error <pre><code># Solution: Reinstall Python bindings\n# pip install maturin\n# maturin develop --force-reinstall\n</code></pre></p> <p>2. Memory Error <pre><code># Solution: Reduce memory usage\nimport os\nos.environ['DPA_MAX_MEMORY'] = '2GB'\n</code></pre></p> <p>3. File Not Found <pre><code># Solution: Check file paths\nimport os\nif not os.path.exists(\"data.csv\"):\n    print(\"File not found!\")\n</code></pre></p>"},{"location":"api/python-api/#debugging","title":"Debugging","text":"<pre><code>import dpa_core\nimport traceback\n\ndef debug_dpa_operation(operation_func, *args, **kwargs):\n    \"\"\"Debug DPA operations with detailed error information.\"\"\"\n    try:\n        return operation_func(*args, **kwargs)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        print(f\"Traceback: {traceback.format_exc()}\")\n        return None\n</code></pre>"},{"location":"api/python-api/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 CLI Commands: Learn about CLI Commands</li> <li>\ud83c\udfaf Examples: See practical examples in the Examples section</li> <li>\ud83d\udd27 Configuration: Understand Configuration Options</li> <li>\ud83d\ude80 Advanced Usage: Explore Best Practices</li> </ul> <p>Ready to use the Python API? Start with the Quick Start Guide to get familiar with the functions.</p>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>Learn how to use DPA for common data processing tasks with practical examples.</p>"},{"location":"examples/basic-usage/#overview","title":"Overview","text":"<p>This guide provides step-by-step examples for the most common DPA operations. Each example includes both CLI and Python API approaches.</p>"},{"location":"examples/basic-usage/#prerequisites","title":"Prerequisites","text":"<p>Before running these examples, ensure you have:</p> <ul> <li>\u2705 DPA installed (see Installation Guide)</li> <li>\u2705 Sample data files (or create your own)</li> <li>\u2705 Basic familiarity with command line or Python</li> </ul>"},{"location":"examples/basic-usage/#example-1-data-profiling","title":"Example 1: Data Profiling","text":""},{"location":"examples/basic-usage/#understanding-your-data","title":"Understanding Your Data","text":"<p>Scenario: You have a CSV file and want to understand its structure and quality.</p>"},{"location":"examples/basic-usage/#cli-approach","title":"CLI Approach","text":"<pre><code># Basic profiling\n./target/release/dpa profile data/transactions.csv\n\n# Detailed profiling with statistics\n./target/release/dpa profile data/transactions.csv --detailed\n\n# Save profile report to file\n./target/release/dpa profile data/transactions.csv --output profile_report.json --format json\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach","title":"Python API Approach","text":"<pre><code>import dpa_core\nimport json\n\n# Basic profiling\nstats = dpa_core.profile_py(\"data/transactions.csv\")\nprint(f\"Total rows: {stats['total_rows']}\")\nprint(f\"Total columns: {stats['total_columns']}\")\nprint(f\"Memory usage: {stats['memory_usage']}\")\n\n# Access column-specific statistics\nfor col_name, col_stats in stats['columns'].items():\n    print(f\"\\n{col_name}:\")\n    print(f\"  Type: {col_stats['type']}\")\n    print(f\"  Null percentage: {col_stats['null_percentage']}%\")\n    print(f\"  Unique values: {col_stats['unique_count']}\")\n\n# Save detailed report\nwith open('profile_report.json', 'w') as f:\n    json.dump(stats, f, indent=2)\n</code></pre> <p>Expected Output: <pre><code>\ud83d\udcca Data Profile Report\n=====================\n\n\ud83d\udcc1 File: data/transactions.csv\n\ud83d\udccf Shape: 10,000 rows \u00d7 5 columns\n\ud83d\udcbe Size: 2.3 MB\n\u23f1\ufe0f  Processing time: 0.8s\n\n\ud83d\udcc8 Column Statistics:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Column  \u2502 Type    \u2502 Null %  \u2502 Unique  \u2502 Min     \u2502 Max     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 id      \u2502 Int64   \u2502 0.0%    \u2502 10,000  \u2502 1       \u2502 10,000  \u2502\n\u2502 amount  \u2502 Float64 \u2502 2.0%    \u2502 8,950   \u2502 10.50   \u2502 9999.99 \u2502\n\u2502 date    \u2502 String  \u2502 0.0%    \u2502 365     \u2502 -       \u2502 -       \u2502\n\u2502 category\u2502 String  \u2502 1.0%    \u2502 15      \u2502 -       \u2502 -       \u2502\n\u2502 country \u2502 String  \u2502 0.0%    \u2502 45      \u2502 -       \u2502 -       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"examples/basic-usage/#example-2-data-validation","title":"Example 2: Data Validation","text":""},{"location":"examples/basic-usage/#ensuring-data-quality","title":"Ensuring Data Quality","text":"<p>Scenario: You want to validate your data against a schema and custom business rules.</p>"},{"location":"examples/basic-usage/#create-schema-file","title":"Create Schema File","text":"<pre><code>{\n  \"columns\": {\n    \"id\": {\n      \"type\": \"integer\",\n      \"nullable\": false,\n      \"unique\": true\n    },\n    \"amount\": {\n      \"type\": \"float\",\n      \"nullable\": true,\n      \"min\": 0.0,\n      \"max\": 10000.0\n    },\n    \"date\": {\n      \"type\": \"string\",\n      \"nullable\": false,\n      \"pattern\": \"\\\\d{4}-\\\\d{2}-\\\\d{2}\"\n    },\n    \"category\": {\n      \"type\": \"string\",\n      \"nullable\": true,\n      \"allowed_values\": [\"electronics\", \"clothing\", \"books\", \"food\"]\n    },\n    \"country\": {\n      \"type\": \"string\",\n      \"nullable\": false,\n      \"min_length\": 2,\n      \"max_length\": 3\n    }\n  }\n}\n</code></pre>"},{"location":"examples/basic-usage/#create-validation-rules","title":"Create Validation Rules","text":"<pre><code>[\n  {\n    \"name\": \"high_value_transactions\",\n    \"column\": \"amount\",\n    \"rule_type\": \"sql\",\n    \"expression\": \"amount &gt; 5000\",\n    \"message\": \"High value transactions detected\",\n    \"severity\": \"warning\"\n  },\n  {\n    \"name\": \"future_dates\",\n    \"column\": \"date\",\n    \"rule_type\": \"sql\",\n    \"expression\": \"date &gt; '2024-12-31'\",\n    \"message\": \"Future dates detected\",\n    \"severity\": \"error\"\n  }\n]\n</code></pre>"},{"location":"examples/basic-usage/#cli-approach_1","title":"CLI Approach","text":"<pre><code># Basic validation\n./target/release/dpa validate data/transactions.csv\n\n# Schema validation\n./target/release/dpa validate data/transactions.csv --schema schema.json\n\n# Custom rules validation\n./target/release/dpa validate data/transactions.csv --rules validation_rules.json\n\n# Combined validation\n./target/release/dpa validate data/transactions.csv --schema schema.json --rules validation_rules.json\n\n# Strict validation (fail on first error)\n./target/release/dpa validate data/transactions.csv --schema schema.json --strict\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_1","title":"Python API Approach","text":"<pre><code>import dpa_core\n\n# Basic validation\ntry:\n    dpa_core.validate_py(\"data/transactions.csv\")\n    print(\"\u2705 Basic validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Validation failed: {e}\")\n\n# Schema validation\ntry:\n    dpa_core.validate_py(\"data/transactions.csv\", schema=\"schema.json\")\n    print(\"\u2705 Schema validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Schema validation failed: {e}\")\n\n# Custom rules validation\ntry:\n    dpa_core.validate_py(\"data/transactions.csv\", rules=\"validation_rules.json\")\n    print(\"\u2705 Rules validation passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Rules validation failed: {e}\")\n\n# Combined validation\ntry:\n    dpa_core.validate_py(\"data/transactions.csv\", schema=\"schema.json\", rules=\"validation_rules.json\")\n    print(\"\u2705 All validations passed!\")\nexcept RuntimeError as e:\n    print(f\"\u274c Validation failed: {e}\")\n</code></pre>"},{"location":"examples/basic-usage/#example-3-data-sampling","title":"Example 3: Data Sampling","text":""},{"location":"examples/basic-usage/#creating-representative-samples","title":"Creating Representative Samples","text":"<p>Scenario: You have a large dataset and want to create smaller samples for analysis or testing.</p>"},{"location":"examples/basic-usage/#cli-approach_2","title":"CLI Approach","text":"<pre><code># Random sampling\n./target/release/dpa sample data/transactions.csv sample_random.csv --method random --size 1000\n\n# Stratified sampling by category\n./target/release/dpa sample data/transactions.csv sample_stratified.csv --method stratified --stratify category --size 500\n\n# Head sampling (first N rows)\n./target/release/dpa sample data/transactions.csv sample_head.csv --method head --size 100\n\n# Tail sampling (last N rows)\n./target/release/dpa sample data/transactions.csv sample_tail.csv --method tail --size 100\n\n# Reproducible sampling with seed\n./target/release/dpa sample data/transactions.csv sample_seeded.csv --method random --size 1000 --seed 42\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_2","title":"Python API Approach","text":"<pre><code>import dpa_core\n\n# Random sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample_random.csv\", size=1000, method=\"random\")\n\n# Stratified sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample_stratified.csv\", size=500, method=\"stratified\", stratify=\"category\")\n\n# Head sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample_head.csv\", size=100, method=\"head\")\n\n# Tail sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample_tail.csv\", size=100, method=\"tail\")\n\n# Reproducible sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample_seeded.csv\", size=1000, method=\"random\", seed=42)\n\n# Verify sample sizes\nimport pandas as pd\noriginal = pd.read_csv(\"data/transactions.csv\")\nsample = pd.read_csv(\"sample_random.csv\")\nprint(f\"Original: {len(original)} rows\")\nprint(f\"Sample: {len(sample)} rows\")\n</code></pre>"},{"location":"examples/basic-usage/#example-4-traintest-split","title":"Example 4: Train/Test Split","text":""},{"location":"examples/basic-usage/#preparing-data-for-machine-learning","title":"Preparing Data for Machine Learning","text":"<p>Scenario: You want to split your data into training and test sets for machine learning.</p>"},{"location":"examples/basic-usage/#cli-approach_3","title":"CLI Approach","text":"<pre><code># Basic split (80% train, 20% test)\n./target/release/dpa split data/transactions.csv train.csv test.csv\n\n# Custom split ratio (70% train, 30% test)\n./target/release/dpa split data/transactions.csv train.csv test.csv --test-size 0.3\n\n# Stratified split by category\n./target/release/dpa split data/transactions.csv train.csv test.csv --stratify category\n\n# Reproducible split with seed\n./target/release/dpa split data/transactions.csv train.csv test.csv --seed 42\n\n# Combined options\n./target/release/dpa split data/transactions.csv train.csv test.csv --test-size 0.25 --stratify category --seed 42\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_3","title":"Python API Approach","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Basic split\ndpa_core.split_py(\"data/transactions.csv\", \"train.csv\", \"test.csv\")\n\n# Custom test size\ndpa_core.split_py(\"data/transactions.csv\", \"train.csv\", \"test.csv\", test_size=0.3)\n\n# Stratified split\ndpa_core.split_py(\"data/transactions.csv\", \"train.csv\", \"test.csv\", stratify=\"category\")\n\n# Reproducible split\ndpa_core.split_py(\"data/transactions.csv\", \"train.csv\", \"test.csv\", seed=42)\n\n# Verify split\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\nprint(f\"Training set: {len(train_df)} rows\")\nprint(f\"Test set: {len(test_df)} rows\")\nprint(f\"Total: {len(train_df) + len(test_df)} rows\")\n\n# Check stratification\nif 'category' in train_df.columns:\n    print(\"\\nCategory distribution in training set:\")\n    print(train_df['category'].value_counts())\n    print(\"\\nCategory distribution in test set:\")\n    print(test_df['category'].value_counts())\n</code></pre>"},{"location":"examples/basic-usage/#example-5-data-filtering","title":"Example 5: Data Filtering","text":""},{"location":"examples/basic-usage/#extracting-subsets-of-data","title":"Extracting Subsets of Data","text":"<p>Scenario: You want to filter your data based on specific conditions.</p>"},{"location":"examples/basic-usage/#cli-approach_4","title":"CLI Approach","text":"<pre><code># Basic filtering\n./target/release/dpa filter data/transactions.csv \"amount &gt; 1000\" filtered_high.csv\n\n# Multiple conditions\n./target/release/dpa filter data/transactions.csv \"amount &gt; 500 AND category = 'electronics'\" filtered_electronics.csv\n\n# Select specific columns with filtering\n./target/release/dpa filter data/transactions.csv \"country = 'US'\" --select \"id,amount,date,category\" filtered_us.csv\n\n# Complex conditions\n./target/release/dpa filter data/transactions.csv \"amount BETWEEN 100 AND 1000 AND date &gt;= '2024-01-01'\" filtered_range.csv\n\n# Output to different format\n./target/release/dpa filter data/transactions.csv \"amount &gt; 500\" filtered.parquet --format parquet\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_4","title":"Python API Approach","text":"<pre><code>import dpa_core\n\n# Basic filtering\noutput = dpa_core.filter_py(\"data/transactions.csv\", \"amount &gt; 1000\", output=\"filtered_high.csv\")\n\n# Multiple conditions\noutput = dpa_core.filter_py(\"data/transactions.csv\", \"amount &gt; 500 AND category = 'electronics'\", output=\"filtered_electronics.csv\")\n\n# Select specific columns\noutput = dpa_core.filter_py(\n    \"data/transactions.csv\", \n    \"country = 'US'\", \n    select=[\"id\", \"amount\", \"date\", \"category\"], \n    output=\"filtered_us.csv\"\n)\n\n# Complex conditions\noutput = dpa_core.filter_py(\n    \"data/transactions.csv\", \n    \"amount BETWEEN 100 AND 1000 AND date &gt;= '2024-01-01'\", \n    output=\"filtered_range.csv\"\n)\n\n# Verify results\nimport pandas as pd\noriginal = pd.read_csv(\"data/transactions.csv\")\nfiltered = pd.read_csv(\"filtered_high.csv\")\nprint(f\"Original: {len(original)} rows\")\nprint(f\"Filtered: {len(filtered)} rows\")\nprint(f\"Filter ratio: {len(filtered)/len(original)*100:.1f}%\")\n</code></pre>"},{"location":"examples/basic-usage/#example-6-column-selection","title":"Example 6: Column Selection","text":""},{"location":"examples/basic-usage/#working-with-specific-columns","title":"Working with Specific Columns","text":"<p>Scenario: You want to select only specific columns from your dataset.</p>"},{"location":"examples/basic-usage/#cli-approach_5","title":"CLI Approach","text":"<pre><code># Select specific columns\n./target/release/dpa select data/transactions.csv \"id,amount,date\" selected_columns.csv\n\n# Select all columns except some\n./target/release/dpa select data/transactions.csv \"!id,!timestamp\" selected_except.csv\n\n# Output to different format\n./target/release/dpa select data/transactions.csv \"amount,date,category\" selected.parquet --format parquet\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_5","title":"Python API Approach","text":"<pre><code>import dpa_core\n\n# Select specific columns\noutput = dpa_core.select_py(\"data/transactions.csv\", [\"id\", \"amount\", \"date\"], output=\"selected_columns.csv\")\n\n# Select all columns except some (using negative selection)\noutput = dpa_core.select_py(\"data/transactions.csv\", [\"!id\", \"!timestamp\"], output=\"selected_except.csv\")\n\n# Verify column selection\nimport pandas as pd\noriginal = pd.read_csv(\"data/transactions.csv\")\nselected = pd.read_csv(\"selected_columns.csv\")\nprint(f\"Original columns: {list(original.columns)}\")\nprint(f\"Selected columns: {list(selected.columns)}\")\n</code></pre>"},{"location":"examples/basic-usage/#example-7-format-conversion","title":"Example 7: Format Conversion","text":""},{"location":"examples/basic-usage/#converting-between-file-formats","title":"Converting Between File Formats","text":"<p>Scenario: You want to convert your data between different file formats for better performance or compatibility.</p>"},{"location":"examples/basic-usage/#cli-approach_6","title":"CLI Approach","text":"<pre><code># CSV to Parquet (better performance)\n./target/release/dpa convert data/transactions.csv data/transactions.parquet\n\n# Parquet to CSV (for compatibility)\n./target/release/dpa convert data/transactions.parquet data/transactions_new.csv\n\n# CSV to JSON\n./target/release/dpa convert data/transactions.csv data/transactions.json\n\n# JSON to Parquet\n./target/release/dpa convert data/transactions.json data/transactions_from_json.parquet\n</code></pre>"},{"location":"examples/basic-usage/#python-api-approach_6","title":"Python API Approach","text":"<pre><code>import dpa_core\nimport time\n\n# CSV to Parquet\nstart_time = time.time()\noutput = dpa_core.convert_py(\"data/transactions.csv\", \"data/transactions.parquet\")\ncsv_to_parquet_time = time.time() - start_time\n\n# Parquet to CSV\nstart_time = time.time()\noutput = dpa_core.convert_py(\"data/transactions.parquet\", \"data/transactions_new.csv\")\nparquet_to_csv_time = time.time() - start_time\n\nprint(f\"CSV to Parquet: {csv_to_parquet_time:.2f}s\")\nprint(f\"Parquet to CSV: {parquet_to_csv_time:.2f}s\")\n\n# Compare file sizes\nimport os\ncsv_size = os.path.getsize(\"data/transactions.csv\") / 1024 / 1024  # MB\nparquet_size = os.path.getsize(\"data/transactions.parquet\") / 1024 / 1024  # MB\nprint(f\"CSV size: {csv_size:.2f} MB\")\nprint(f\"Parquet size: {parquet_size:.2f} MB\")\nprint(f\"Compression ratio: {csv_size/parquet_size:.1f}x\")\n</code></pre>"},{"location":"examples/basic-usage/#example-8-complete-data-pipeline","title":"Example 8: Complete Data Pipeline","text":""},{"location":"examples/basic-usage/#end-to-end-data-processing","title":"End-to-End Data Processing","text":"<p>Scenario: You want to create a complete data processing pipeline from raw data to ML-ready datasets.</p>"},{"location":"examples/basic-usage/#pipeline-steps","title":"Pipeline Steps","text":"<ol> <li>Profile the data to understand its structure</li> <li>Validate the data against schemas and rules</li> <li>Clean the data by filtering invalid records</li> <li>Sample the data for faster processing</li> <li>Split the data for machine learning</li> <li>Convert formats for optimal performance</li> </ol>"},{"location":"examples/basic-usage/#cli-pipeline","title":"CLI Pipeline","text":"<pre><code>#!/bin/bash\n\necho \"Starting DPA Data Pipeline\"\n\n# 1. Profile the data\necho \"Profiling data...\"\n./target/release/dpa profile data/raw_transactions.csv --output profile_report.json --format json\n\n# 2. Validate the data\necho \"Validating data...\"\n./target/release/dpa validate data/raw_transactions.csv --schema schema.json --rules validation_rules.json\n\n# 3. Filter out invalid records\necho \"Cleaning data...\"\n./target/release/dpa filter data/raw_transactions.csv \"amount &gt; 0 AND date IS NOT NULL\" data/clean_transactions.csv\n\n# 4. Sample for faster processing\necho \"Sampling data...\"\n./target/release/dpa sample data/clean_transactions.csv data/sample_transactions.csv --method stratified --stratify category --size 10000\n\n# 5. Split for machine learning\necho \"Splitting data...\"\n./target/release/dpa split data/sample_transactions.csv data/train.csv data/test.csv --test-size 0.2 --stratify category\n\n# 6. Convert to efficient format\necho \"Converting formats...\"\n./target/release/dpa convert data/train.csv data/train.parquet\n./target/release/dpa convert data/test.csv data/test.parquet\n\necho \"Pipeline completed successfully!\"\n</code></pre>"},{"location":"examples/basic-usage/#python-pipeline","title":"Python Pipeline","text":"<pre><code>import dpa_core\nimport json\nimport time\nfrom pathlib import Path\n\ndef data_pipeline():\n    \"\"\"Complete data processing pipeline.\"\"\"\n\n    print(\"Starting DPA Data Pipeline\")\n\n    # Create output directory\n    Path(\"data/processed\").mkdir(exist_ok=True)\n\n    # 1. Profile the data\n    print(\"Profiling data...\")\n    start_time = time.time()\n    stats = dpa_core.profile_py(\"data/raw_transactions.csv\")\n    profile_time = time.time() - start_time\n\n    with open(\"data/processed/profile_report.json\", \"w\") as f:\n        json.dump(stats, f, indent=2)\n\n    print(f\"   Profiling completed in {profile_time:.2f}s\")\n    print(f\"   Found {stats['total_rows']} rows and {stats['total_columns']} columns\")\n\n    # 2. Validate the data\n    print(\"Validating data...\")\n    try:\n        dpa_core.validate_py(\"data/raw_transactions.csv\", schema=\"schema.json\", rules=\"validation_rules.json\")\n        print(\"   Validation passed!\")\n    except RuntimeError as e:\n        print(f\"   Validation warnings: {e}\")\n\n    # 3. Clean the data\n    print(\"Cleaning data...\")\n    dpa_core.filter_py(\"data/raw_transactions.csv\", \"amount &gt; 0 AND date IS NOT NULL\", output=\"data/processed/clean_transactions.csv\")\n\n    # 4. Sample the data\n    print(\"Sampling data...\")\n    dpa_core.sample_py(\"data/processed/clean_transactions.csv\", \"data/processed/sample_transactions.csv\", \n                      size=10000, method=\"stratified\", stratify=\"category\")\n\n    # 5. Split for ML\n    print(\"Splitting data...\")\n    dpa_core.split_py(\"data/processed/sample_transactions.csv\", \n                     \"data/processed/train.csv\", \"data/processed/test.csv\", \n                     test_size=0.2, stratify=\"category\")\n\n    # 6. Convert to efficient format\n    print(\"Converting formats...\")\n    dpa_core.convert_py(\"data/processed/train.csv\", \"data/processed/train.parquet\")\n    dpa_core.convert_py(\"data/processed/test.csv\", \"data/processed/test.parquet\")\n\n    # Generate summary report\n    summary = {\n        \"pipeline_completed\": True,\n        \"processing_time\": time.time() - start_time,\n        \"output_files\": [\n            \"data/processed/profile_report.json\",\n            \"data/processed/clean_transactions.csv\",\n            \"data/processed/sample_transactions.csv\",\n            \"data/processed/train.csv\",\n            \"data/processed/test.csv\",\n            \"data/processed/train.parquet\",\n            \"data/processed/test.parquet\"\n        ],\n        \"statistics\": stats\n    }\n\n    with open(\"data/processed/pipeline_summary.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n\n    print(\"Pipeline completed successfully!\")\n    print(f\"Total processing time: {summary['processing_time']:.2f}s\")\n\n    return summary\n\n# Run the pipeline\nif __name__ == \"__main__\":\n    result = data_pipeline()\n</code></pre>"},{"location":"examples/basic-usage/#best-practices","title":"Best Practices","text":""},{"location":"examples/basic-usage/#1-start-small","title":"1. Start Small","text":"<ul> <li>Begin with small datasets to understand the tools</li> <li>Use sampling for large files during development</li> <li>Test commands on subsets before processing large files</li> </ul>"},{"location":"examples/basic-usage/#2-use-appropriate-formats","title":"2. Use Appropriate Formats","text":"<ul> <li>Use Parquet for large datasets (better compression and performance)</li> <li>Use CSV for compatibility with other tools</li> <li>Use JSON for web APIs and configuration</li> </ul>"},{"location":"examples/basic-usage/#3-monitor-performance","title":"3. Monitor Performance","text":"<ul> <li>Use the <code>--detailed</code> flag sparingly on large datasets</li> <li>Monitor memory usage with large files</li> <li>Use sampling for profiling large datasets</li> </ul>"},{"location":"examples/basic-usage/#4-validate-your-data","title":"4. Validate Your Data","text":"<ul> <li>Always validate data before processing</li> <li>Use schemas to ensure data quality</li> <li>Create custom validation rules for business logic</li> </ul>"},{"location":"examples/basic-usage/#5-reproducible-results","title":"5. Reproducible Results","text":"<ul> <li>Use seeds for random operations</li> <li>Document your pipeline steps</li> <li>Save intermediate results for debugging</li> </ul>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Examples: Explore Data Quality examples</li> <li>ML Integration: See Machine Learning examples</li> <li>Performance: Learn Performance Optimization techniques</li> <li>API Reference: Check the complete API documentation</li> </ul> <p>Ready to build your own data processing pipelines? Start with these examples and customize them for your specific needs!</p>"},{"location":"features/data-profiling/","title":"Data Profiling","text":"<p>DPA provides comprehensive data profiling capabilities that go beyond simple statistics to give you deep insights into your data quality, structure, and characteristics.</p>"},{"location":"features/data-profiling/#overview","title":"Overview","text":"<p>The enhanced profiling feature analyzes your data and provides:</p> <ul> <li>Basic Statistics: Row/column counts, memory usage, data types</li> <li>Data Quality Metrics: Null percentages, unique value counts</li> <li>Statistical Summaries: Min, max, mean, standard deviation, percentiles</li> <li>Value Distributions: Most common values, average string lengths</li> <li>Outlier Detection: Statistical outliers and anomalies</li> </ul>"},{"location":"features/data-profiling/#command-line-usage","title":"Command Line Usage","text":""},{"location":"features/data-profiling/#basic-profiling","title":"Basic Profiling","text":"<pre><code># Basic profile with default settings\ndpa profile data/transactions.csv\n\n# Profile with custom sample size\ndpa profile data/transactions.csv --sample 500000\n\n# Detailed profiling with statistics\ndpa profile data/transactions.csv --detailed\n</code></pre>"},{"location":"features/data-profiling/#options","title":"Options","text":"Option Description Default <code>--sample, -s</code> Sample size for profiling 1,000,000 <code>--detailed, -d</code> Show detailed statistics false"},{"location":"features/data-profiling/#python-api","title":"Python API","text":"<pre><code>import dpa_core\n\n# Basic profiling\nprofile = dpa_core.profile_py(\"data/transactions.csv\")\n\n# Access profiling results\nprint(f\"Rows: {profile['rows']}\")\nprint(f\"Columns: {profile['columns']}\")\nprint(f\"Memory Usage: {profile['memory_mb']} MB\")\nprint(f\"Null Percentage: {profile['null_percentage']}%\")\n\n# Column-specific information\nfor col in ['user_id', 'amount', 'country']:\n    dtype_key = f\"dtype:{col}\"\n    nulls_key = f\"nulls:{col}\"\n    unique_key = f\"unique:{col}\"\n\n    if dtype_key in profile:\n        print(f\"{col}: {profile[dtype_key]}, nulls={profile[nulls_key]}, unique={profile[unique_key]}\")\n</code></pre>"},{"location":"features/data-profiling/#output-format","title":"Output Format","text":""},{"location":"features/data-profiling/#basic-profile-output","title":"Basic Profile Output","text":"<pre><code>\ud83d\udcca Data Profile Report\n==================================================\n\ud83d\udcc1 File: data/transactions.csv\n\ud83d\udcc8 Total Rows: 501\n\ud83d\udccb Total Columns: 5\n\ud83d\udcbe Memory Usage: ~0.02 MB\n\n\ud83d\udccb Column Overview:\nColumn               Type         Nulls      Null %    Unique    \n----------------------------------------------------------------------\nuser_id             Int64        0          0.0%      501       \namount              Float64      0          0.0%      501       \ncountry             Utf8         0          0.0%      6         \ntimestamp           Int64        0          0.0%      501       \nchannel             Utf8         0          0.0%      3         \n\n\ud83d\udd0d Data Quality Summary:\n==================================================\n\ud83d\udcca Overall Null Percentage: 0.00%\n\ud83d\udd22 Total Null Values: 0\n\u2705 Complete Rows: 501\n</code></pre>"},{"location":"features/data-profiling/#detailed-profile-output","title":"Detailed Profile Output","text":"<p>When using <code>--detailed</code>, you get additional statistical information:</p> <pre><code>\ud83d\udcca Detailed Statistics:\n==================================================\n\n\ud83d\udd22 amount (Numeric):\n   Min: 2.9400\n   Max: 167.5800\n   Mean: 37.2347\n   Std: 32.4567\n   Q1 (25%): 15.5300\n   Q3 (75%): 51.4100\n   IQR: 35.8800\n\n\ud83d\udcdd country (Text):\n   Avg Length: 2.0 chars\n   Top 5 Values:\n     'US': 125 times\n     'IT': 98 times\n     'DE': 87 times\n     'ES': 85 times\n     'NL': 67 times\n</code></pre>"},{"location":"features/data-profiling/#use-cases","title":"Use Cases","text":""},{"location":"features/data-profiling/#1-data-quality-assessment","title":"1. Data Quality Assessment","text":"<pre><code># Quick quality check\ndpa profile data/dataset.csv | grep \"Null %\"\n</code></pre>"},{"location":"features/data-profiling/#2-memory-usage-optimization","title":"2. Memory Usage Optimization","text":"<pre><code># Check memory usage before processing large files\ndpa profile large_file.csv | grep \"Memory Usage\"\n</code></pre>"},{"location":"features/data-profiling/#3-schema-discovery","title":"3. Schema Discovery","text":"<pre><code># Understand data structure\ndpa profile new_dataset.csv --detailed\n</code></pre>"},{"location":"features/data-profiling/#4-outlier-detection","title":"4. Outlier Detection","text":"<pre><code># Find potential data issues\ndpa profile data.csv --detailed | grep -A 10 \"outliers\"\n</code></pre>"},{"location":"features/data-profiling/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Sampling: Large files are automatically sampled (default: 1M rows)</li> <li>Memory Efficient: Uses lazy evaluation for large datasets</li> <li>Parallel Processing: Statistical calculations are parallelized</li> </ul>"},{"location":"features/data-profiling/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"features/data-profiling/#1-choose-appropriate-sample-size","title":"1. Choose Appropriate Sample Size","text":"<pre><code># For large files, use smaller samples for quick analysis\ndpa profile huge_file.csv --sample 100000\n\n# For small files, use full dataset\ndpa profile small_file.csv --sample 1000000\n</code></pre>"},{"location":"features/data-profiling/#2-use-detailed-mode-for-analysis","title":"2. Use Detailed Mode for Analysis","text":"<pre><code># Always use --detailed for data exploration\ndpa profile data.csv --detailed &gt; profile_report.txt\n</code></pre>"},{"location":"features/data-profiling/#3-monitor-memory-usage","title":"3. Monitor Memory Usage","text":"<pre><code># Check memory before processing\ndpa profile data.csv | grep \"Memory\"\n</code></pre>"},{"location":"features/data-profiling/#4-validate-data-types","title":"4. Validate Data Types","text":"<pre><code># Check for mixed data types\ndpa profile data.csv --detailed | grep \"mixed_types\"\n</code></pre>"},{"location":"features/data-profiling/#integration-with-other-tools","title":"Integration with Other Tools","text":""},{"location":"features/data-profiling/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Profile data and create pandas DataFrame\nprofile = dpa_core.profile_py(\"data.csv\")\nprofile_df = pd.DataFrame(list(profile.items()), columns=['Metric', 'Value'])\nprofile_df\n</code></pre>"},{"location":"features/data-profiling/#data-validation-pipeline","title":"Data Validation Pipeline","text":"<pre><code># Profile first, then validate\ndpa profile data.csv --detailed\ndpa validate data.csv --schema schema.json\n</code></pre>"},{"location":"features/data-profiling/#etl-workflows","title":"ETL Workflows","text":"<pre><code># Profile before and after transformations\ndpa profile raw_data.csv &gt; before_profile.txt\ndpa filter raw_data.csv -w \"amount &gt; 0\" -o clean_data.csv\ndpa profile clean_data.csv &gt; after_profile.txt\n</code></pre>"},{"location":"features/data-profiling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/data-profiling/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Memory Errors: Reduce sample size    <pre><code>dpa profile large_file.csv --sample 100000\n</code></pre></p> </li> <li> <p>Slow Performance: Use basic profiling for large files    <pre><code>dpa profile large_file.csv  # Skip --detailed\n</code></pre></p> </li> <li> <p>Encoding Issues: Ensure proper file encoding    <pre><code># Check file encoding first\nfile -i data.csv\n</code></pre></p> </li> </ol>"},{"location":"features/data-profiling/#error-messages","title":"Error Messages","text":"<ul> <li><code>\"File not found\"</code>: Check file path and permissions</li> <li><code>\"Invalid sample size\"</code>: Use positive integers</li> <li><code>\"Memory allocation failed\"</code>: Reduce sample size or use smaller files</li> </ul>"},{"location":"features/data-profiling/#advanced-features","title":"Advanced Features","text":""},{"location":"features/data-profiling/#custom-profiling-scripts","title":"Custom Profiling Scripts","text":"<pre><code>import dpa_core\nimport json\n\ndef custom_profile(file_path):\n    \"\"\"Custom profiling with specific metrics\"\"\"\n    profile = dpa_core.profile_py(file_path)\n\n    # Extract specific metrics\n    metrics = {\n        'file_size_mb': profile.get('memory_mb', 0),\n        'total_rows': int(profile.get('rows', 0)),\n        'null_percentage': float(profile.get('null_percentage', 0)),\n        'columns': int(profile.get('columns', 0))\n    }\n\n    return metrics\n\n# Usage\nmetrics = custom_profile(\"data.csv\")\nprint(json.dumps(metrics, indent=2))\n</code></pre>"},{"location":"features/data-profiling/#batch-profiling","title":"Batch Profiling","text":"<pre><code>#!/bin/bash\n# Profile multiple files\nfor file in data/*.csv; do\n    echo \"Profiling $file...\"\n    dpa profile \"$file\" --detailed &gt; \"profiles/$(basename \"$file\" .csv)_profile.txt\"\ndone\n</code></pre> <p>This enhanced profiling feature provides the foundation for understanding your data quality and structure, enabling better decision-making in data processing workflows.</p>"},{"location":"features/data-sampling/","title":"Data Sampling","text":"<p>DPA provides powerful data sampling capabilities for data exploration, testing, and machine learning workflows. The sampling feature supports multiple methods including random sampling, stratified sampling, and systematic sampling.</p>"},{"location":"features/data-sampling/#overview","title":"Overview","text":"<p>The data sampling feature includes:</p> <ul> <li>Random Sampling: Simple random sampling with optional seeding</li> <li>Stratified Sampling: Maintains distribution of key columns</li> <li>Systematic Sampling: Head, tail, and systematic sampling</li> <li>Reproducible Results: Seeded random sampling for consistent results</li> <li>Performance Optimized: Efficient sampling for large datasets</li> </ul>"},{"location":"features/data-sampling/#command-line-usage","title":"Command Line Usage","text":""},{"location":"features/data-sampling/#basic-sampling","title":"Basic Sampling","text":"<pre><code># Random sampling (default)\ndpa sample data/transactions.csv -o sample.csv --size 1000\n\n# Stratified sampling\ndpa sample data/transactions.csv -o sample.csv --method stratified --stratify country --size 500\n\n# Head sampling (first N rows)\ndpa sample data/transactions.csv -o sample.csv --method head --size 100\n\n# Tail sampling (last N rows)\ndpa sample data/transactions.csv -o sample.csv --method tail --size 100\n</code></pre>"},{"location":"features/data-sampling/#options","title":"Options","text":"Option Description Default <code>--size, -s</code> Sample size 1000 <code>--method, -m</code> Sampling method: random, stratified, head, tail random <code>--stratify</code> Column to stratify by (required for stratified) None <code>--seed</code> Random seed for reproducible sampling None"},{"location":"features/data-sampling/#python-api","title":"Python API","text":"<pre><code>import dpa_core\n\n# Random sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample.csv\", size=1000)\n\n# Stratified sampling\ndpa_core.sample_py(\"data/transactions.csv\", \"sample.csv\", method=\"stratified\", stratify=\"country\", size=500)\n\n# With seed for reproducibility\ndpa_core.sample_py(\"data/transactions.csv\", \"sample.csv\", size=1000, seed=42)\n</code></pre>"},{"location":"features/data-sampling/#sampling-methods","title":"Sampling Methods","text":""},{"location":"features/data-sampling/#1-random-sampling","title":"1. Random Sampling","text":"<p>Simple random sampling without replacement:</p> <pre><code># Basic random sampling\ndpa sample data/transactions.csv -o random_sample.csv --size 1000\n\n# With seed for reproducibility\ndpa sample data/transactions.csv -o random_sample.csv --size 1000 --seed 42\n</code></pre>"},{"location":"features/data-sampling/#2-stratified-sampling","title":"2. Stratified Sampling","text":"<p>Maintains the distribution of a categorical column:</p> <pre><code># Stratified by country\ndpa sample data/transactions.csv -o stratified_sample.csv --method stratified --stratify country --size 500\n\n# Stratified by channel\ndpa sample data/transactions.csv -o stratified_sample.csv --method stratified --stratify channel --size 300\n</code></pre>"},{"location":"features/data-sampling/#3-head-sampling","title":"3. Head Sampling","text":"<p>Takes the first N rows:</p> <pre><code># First 100 rows\ndpa sample data/transactions.csv -o head_sample.csv --method head --size 100\n</code></pre>"},{"location":"features/data-sampling/#4-tail-sampling","title":"4. Tail Sampling","text":"<p>Takes the last N rows:</p> <pre><code># Last 100 rows\ndpa sample data/transactions.csv -o tail_sample.csv --method tail --size 100\n</code></pre>"},{"location":"features/data-sampling/#data-splitting","title":"Data Splitting","text":"<p>DPA also provides data splitting functionality for machine learning workflows:</p>"},{"location":"features/data-sampling/#traintest-split","title":"Train/Test Split","text":"<pre><code># Basic 80/20 split\ndpa split data/transactions.csv --train train.csv --test test.csv --test-size 0.2\n\n# Stratified split\ndpa split data/transactions.csv --train train.csv --test test.csv --test-size 0.2 --stratify country\n\n# With seed for reproducibility\ndpa split data/transactions.csv --train train.csv --test test.csv --test-size 0.2 --seed 42\n</code></pre>"},{"location":"features/data-sampling/#split-options","title":"Split Options","text":"Option Description Default <code>--train</code> Output file for training data Required <code>--test</code> Output file for test data Required <code>--test-size, -t</code> Proportion for test set (0.0-1.0) 0.2 <code>--stratify</code> Column to stratify by None <code>--seed</code> Random seed for reproducible split None"},{"location":"features/data-sampling/#use-cases","title":"Use Cases","text":""},{"location":"features/data-sampling/#1-data-exploration","title":"1. Data Exploration","text":"<pre><code># Quick exploration with small sample\ndpa sample large_dataset.csv -o explore_sample.csv --size 1000\ndpa profile explore_sample.csv --detailed\n</code></pre>"},{"location":"features/data-sampling/#2-machine-learning","title":"2. Machine Learning","text":"<pre><code># Stratified sampling for balanced dataset\ndpa sample imbalanced_data.csv -o balanced_sample.csv --method stratified --stratify target --size 5000\n\n# Train/test split\ndpa split ml_data.csv --train train.csv --test test.csv --test-size 0.2 --stratify target\n</code></pre>"},{"location":"features/data-sampling/#3-testing-and-development","title":"3. Testing and Development","text":"<pre><code># Small sample for testing\ndpa sample production_data.csv -o test_data.csv --size 100 --seed 123\n\n# Reproducible development data\ndpa sample large_file.csv -o dev_sample.csv --size 1000 --seed 42\n</code></pre>"},{"location":"features/data-sampling/#4-performance-testing","title":"4. Performance Testing","text":"<pre><code># Different sample sizes for performance testing\nfor size in 1000 5000 10000 50000; do\n    dpa sample large_file.csv -o \"sample_${size}.csv\" --size $size\ndone\n</code></pre>"},{"location":"features/data-sampling/#best-practices","title":"Best Practices","text":""},{"location":"features/data-sampling/#1-choose-appropriate-sample-size","title":"1. Choose Appropriate Sample Size","text":"<pre><code># For exploration: 1K-10K rows\ndpa sample data.csv -o explore.csv --size 5000\n\n# For testing: 100-1K rows\ndpa sample data.csv -o test.csv --size 500\n\n# For ML: 5K-50K rows\ndpa sample data.csv -o ml_sample.csv --size 20000\n</code></pre>"},{"location":"features/data-sampling/#2-use-stratified-sampling-for-imbalanced-data","title":"2. Use Stratified Sampling for Imbalanced Data","text":"<pre><code># Maintain class distribution\ndpa sample imbalanced.csv -o balanced.csv --method stratified --stratify class --size 5000\n</code></pre>"},{"location":"features/data-sampling/#3-use-seeds-for-reproducibility","title":"3. Use Seeds for Reproducibility","text":"<pre><code># Always use seeds in production\ndpa sample data.csv -o sample.csv --size 1000 --seed 42\n</code></pre>"},{"location":"features/data-sampling/#4-validate-sample-quality","title":"4. Validate Sample Quality","text":"<pre><code># Check sample distribution\ndpa sample data.csv -o sample.csv --method stratified --stratify country --size 1000\ndpa profile sample.csv --detailed\n</code></pre>"},{"location":"features/data-sampling/#performance-considerations","title":"Performance Considerations","text":""},{"location":"features/data-sampling/#1-large-file-sampling","title":"1. Large File Sampling","text":"<pre><code># For very large files, use smaller samples\ndpa sample huge_file.csv -o sample.csv --size 10000\n</code></pre>"},{"location":"features/data-sampling/#2-memory-usage","title":"2. Memory Usage","text":"<pre><code># Check memory before sampling large files\ndpa profile large_file.csv | grep \"Memory\"\n</code></pre>"},{"location":"features/data-sampling/#3-stratified-sampling-performance","title":"3. Stratified Sampling Performance","text":"<pre><code># Stratified sampling is slower but more accurate\n# Use for important analyses, not for quick exploration\ndpa sample data.csv -o sample.csv --method stratified --stratify category --size 1000\n</code></pre>"},{"location":"features/data-sampling/#integration-examples","title":"Integration Examples","text":""},{"location":"features/data-sampling/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Sample data for analysis\ndpa_core.sample_py(\"large_data.csv\", \"sample.csv\", size=5000, seed=42)\n\n# Load sample for analysis\nsample_df = pd.read_csv(\"sample.csv\")\nprint(f\"Sample shape: {sample_df.shape}\")\n\n# Analyze sample\ndpa_core.profile_py(\"sample.csv\")\n</code></pre>"},{"location":"features/data-sampling/#machine-learning-pipeline","title":"Machine Learning Pipeline","text":"<pre><code>import dpa_core\nfrom sklearn.model_selection import train_test_split\n\n# Create train/test split\ndpa_core.split_py(\"ml_data.csv\", \"train.csv\", \"test.csv\", test_size=0.2, stratify=\"target\", seed=42)\n\n# Load data\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\n\nprint(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n</code></pre>"},{"location":"features/data-sampling/#automated-sampling","title":"Automated Sampling","text":"<pre><code>#!/bin/bash\n# Automated sampling pipeline\nfor file in data/*.csv; do\n    basename=$(basename \"$file\" .csv)\n\n    # Create samples of different sizes\n    for size in 1000 5000 10000; do\n        dpa sample \"$file\" -o \"samples/${basename}_sample_${size}.csv\" --size $size --seed 42\n    done\n\n    # Create stratified sample\n    dpa sample \"$file\" -o \"samples/${basename}_stratified.csv\" --method stratified --stratify country --size 5000 --seed 42\ndone\n</code></pre>"},{"location":"features/data-sampling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/data-sampling/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Stratified Sampling Errors <pre><code># Ensure stratification column exists\ndpa schema data.csv | grep country\n</code></pre></p> </li> <li> <p>Sample Size Too Large <pre><code># Check total rows first\ndpa profile data.csv | grep \"Total Rows\"\n</code></pre></p> </li> <li> <p>Memory Issues <pre><code># Use smaller samples for large files\ndpa sample large_file.csv -o sample.csv --size 1000\n</code></pre></p> </li> </ol>"},{"location":"features/data-sampling/#error-messages","title":"Error Messages","text":"<ul> <li><code>\"--stratify column required\"</code>: Specify stratification column</li> <li><code>\"Sample size exceeds total rows\"</code>: Reduce sample size</li> <li><code>\"Column not found\"</code>: Check column names</li> </ul>"},{"location":"features/data-sampling/#advanced-features","title":"Advanced Features","text":""},{"location":"features/data-sampling/#custom-sampling-scripts","title":"Custom Sampling Scripts","text":"<pre><code>import dpa_core\nimport pandas as pd\n\ndef progressive_sampling(file_path, sizes=[1000, 5000, 10000]):\n    \"\"\"Create progressive samples for analysis\"\"\"\n    samples = {}\n\n    for size in sizes:\n        output_file = f\"sample_{size}.csv\"\n        dpa_core.sample_py(file_path, output_file, size=size, seed=42)\n        samples[size] = pd.read_csv(output_file)\n\n    return samples\n\n# Usage\nsamples = progressive_sampling(\"data.csv\")\nfor size, df in samples.items():\n    print(f\"Sample {size}: {len(df)} rows\")\n</code></pre>"},{"location":"features/data-sampling/#quality-assessment","title":"Quality Assessment","text":"<pre><code>import dpa_core\n\ndef assess_sample_quality(original_file, sample_file, stratify_col=None):\n    \"\"\"Assess sample quality compared to original\"\"\"\n\n    # Profile both files\n    original_profile = dpa_core.profile_py(original_file)\n    sample_profile = dpa_core.profile_py(sample_file)\n\n    # Compare distributions\n    if stratify_col:\n        print(f\"Original {stratify_col} distribution:\")\n        # Add distribution comparison logic\n\n    print(f\"Sample represents {sample_profile['rows']}/{original_profile['rows']} rows\")\n\n    return sample_profile\n\n# Usage\nassess_sample_quality(\"data.csv\", \"sample.csv\", \"country\")\n</code></pre> <p>This comprehensive sampling system provides the tools needed for effective data exploration, testing, and machine learning workflows.</p>"},{"location":"features/data-validation/","title":"Data Validation","text":"<p>DPA provides comprehensive data validation capabilities to ensure data quality, schema compliance, and business rule enforcement. The validation system can detect issues ranging from simple data type mismatches to complex business logic violations.</p>"},{"location":"features/data-validation/#overview","title":"Overview","text":"<p>The data validation feature includes:</p> <ul> <li>Schema Validation: Verify column types and structure</li> <li>Data Type Validation: Detect mixed data types and type inconsistencies</li> <li>Range Validation: Check numeric ranges and detect outliers</li> <li>Custom Rules: User-defined validation rules using SQL expressions</li> <li>Quality Reporting: Detailed reports with error and warning counts</li> </ul>"},{"location":"features/data-validation/#command-line-usage","title":"Command Line Usage","text":""},{"location":"features/data-validation/#basic-validation","title":"Basic Validation","text":"<pre><code># Basic validation (automatic checks)\ndpa validate data/transactions.csv\n\n# Validation with schema file\ndpa validate data/transactions.csv --schema schema.json\n\n# Validation with custom rules\ndpa validate data/transactions.csv --rules validation_rules.json\n\n# Complete validation with output\ndpa validate data/transactions.csv --schema schema.json --rules rules.json -o invalid_rows.csv\n</code></pre>"},{"location":"features/data-validation/#options","title":"Options","text":"Option Description Required <code>--schema</code> JSON file with expected column types No <code>--rules</code> JSON file with custom validation rules No <code>--output, -o</code> Output file for invalid rows No"},{"location":"features/data-validation/#python-api","title":"Python API","text":"<pre><code>import dpa_core\n\n# Basic validation\ndpa_core.validate_py(\"data/transactions.csv\")\n\n# Validation with schema and rules\ndpa_core.validate_py(\"data/transactions.csv\", \"schema.json\", \"rules.json\")\n</code></pre>"},{"location":"features/data-validation/#schema-validation","title":"Schema Validation","text":""},{"location":"features/data-validation/#schema-file-format","title":"Schema File Format","text":"<p>Create a JSON file defining expected column types:</p> <pre><code>{\n  \"user_id\": \"Int64\",\n  \"amount\": \"Float64\",\n  \"country\": \"Utf8\",\n  \"timestamp\": \"Int64\",\n  \"channel\": \"Utf8\"\n}\n</code></pre>"},{"location":"features/data-validation/#usage","title":"Usage","text":"<pre><code># Validate against schema\ndpa validate data/transactions.csv --schema expected_schema.json\n</code></pre>"},{"location":"features/data-validation/#custom-validation-rules","title":"Custom Validation Rules","text":""},{"location":"features/data-validation/#rules-file-format","title":"Rules File Format","text":"<p>Create a JSON file with custom validation rules:</p> <pre><code>[\n  {\n    \"name\": \"positive_amounts\",\n    \"column\": \"amount\",\n    \"rule_type\": \"sql\",\n    \"expression\": \"amount &lt;= 0\",\n    \"message\": \"Amount must be positive\",\n    \"severity\": \"error\"\n  },\n  {\n    \"name\": \"valid_countries\",\n    \"column\": \"country\",\n    \"rule_type\": \"sql\",\n    \"expression\": \"country NOT IN ('US', 'IT', 'DE', 'ES', 'NL', 'GB', 'FR')\",\n    \"message\": \"Invalid country code\",\n    \"severity\": \"error\"\n  },\n  {\n    \"name\": \"amount_range\",\n    \"column\": \"amount\",\n    \"rule_type\": \"range\",\n    \"expression\": \"0.01,1000.0\",\n    \"message\": \"Amount must be between 0.01 and 1000.0\",\n    \"severity\": \"warning\"\n  }\n]\n</code></pre>"},{"location":"features/data-validation/#rule-types","title":"Rule Types","text":""},{"location":"features/data-validation/#1-sql-rules","title":"1. SQL Rules","text":"<p>Use SQL expressions to validate data:</p> <pre><code>{\n  \"name\": \"future_timestamps\",\n  \"column\": \"timestamp\",\n  \"rule_type\": \"sql\",\n  \"expression\": \"timestamp &gt; 1750000000\",\n  \"message\": \"Timestamp is in the future\",\n  \"severity\": \"error\"\n}\n</code></pre>"},{"location":"features/data-validation/#2-range-rules","title":"2. Range Rules","text":"<p>Validate numeric ranges:</p> <pre><code>{\n  \"name\": \"age_range\",\n  \"column\": \"age\",\n  \"rule_type\": \"range\",\n  \"expression\": \"0,120\",\n  \"message\": \"Age must be between 0 and 120\",\n  \"severity\": \"error\"\n}\n</code></pre>"},{"location":"features/data-validation/#output-format","title":"Output Format","text":""},{"location":"features/data-validation/#validation-report","title":"Validation Report","text":"<pre><code>\ud83d\udd0d Data Validation Report\n============================================================\n\u274c Errors (2):\n   \u2022 amount: 5 negative values found in column that should be positive (5 invalid values)\n   \u2022 country: Invalid country code (3 invalid values)\n\n\u26a0\ufe0f  Warnings (1):\n   \u2022 amount: 12 outliers detected (beyond 3\u03c3 from mean) (12 affected values)\n\n\ud83d\udcca Summary: 2 errors, 1 warnings\n</code></pre>"},{"location":"features/data-validation/#success-report","title":"Success Report","text":"<pre><code>\u2705 All validations passed!\n</code></pre>"},{"location":"features/data-validation/#automatic-validations","title":"Automatic Validations","text":"<p>DPA automatically performs these validations:</p>"},{"location":"features/data-validation/#1-data-type-detection","title":"1. Data Type Detection","text":"<ul> <li>Mixed Types: Detects when string columns contain mostly numeric or date values</li> <li>Type Suggestions: Recommends appropriate data types</li> </ul>"},{"location":"features/data-validation/#2-range-validation","title":"2. Range Validation","text":"<ul> <li>Outlier Detection: Identifies statistical outliers (beyond 3\u03c3)</li> <li>Negative Values: Checks for negative values in columns that should be positive</li> <li>Domain Validation: Validates against common business rules</li> </ul>"},{"location":"features/data-validation/#3-data-quality-checks","title":"3. Data Quality Checks","text":"<ul> <li>Null Analysis: Reports null percentages and patterns</li> <li>Unique Value Analysis: Identifies potential issues with cardinality</li> <li>Value Distribution: Analyzes value distributions for anomalies</li> </ul>"},{"location":"features/data-validation/#use-cases","title":"Use Cases","text":""},{"location":"features/data-validation/#1-data-quality-assurance","title":"1. Data Quality Assurance","text":"<pre><code># Validate incoming data\ndpa validate new_data.csv --schema production_schema.json --rules business_rules.json\n</code></pre>"},{"location":"features/data-validation/#2-etl-pipeline-validation","title":"2. ETL Pipeline Validation","text":"<pre><code># Validate transformed data\ndpa validate cleaned_data.csv --rules transformation_rules.json -o validation_errors.csv\n</code></pre>"},{"location":"features/data-validation/#3-schema-migration","title":"3. Schema Migration","text":"<pre><code># Validate data against new schema\ndpa validate legacy_data.csv --schema new_schema.json\n</code></pre>"},{"location":"features/data-validation/#4-data-monitoring","title":"4. Data Monitoring","text":"<pre><code># Daily data quality check\ndpa validate daily_export.csv --rules monitoring_rules.json\n</code></pre>"},{"location":"features/data-validation/#best-practices","title":"Best Practices","text":""},{"location":"features/data-validation/#1-schema-design","title":"1. Schema Design","text":"<pre><code>{\n  \"user_id\": \"Int64\",\n  \"email\": \"Utf8\",\n  \"created_at\": \"Int64\",\n  \"status\": \"Utf8\",\n  \"score\": \"Float64\"\n}\n</code></pre>"},{"location":"features/data-validation/#2-rule-design","title":"2. Rule Design","text":"<pre><code>[\n  {\n    \"name\": \"email_format\",\n    \"column\": \"email\",\n    \"rule_type\": \"sql\",\n    \"expression\": \"email NOT LIKE '%@%.%'\",\n    \"message\": \"Invalid email format\",\n    \"severity\": \"error\"\n  },\n  {\n    \"name\": \"positive_scores\",\n    \"column\": \"score\",\n    \"rule_type\": \"range\",\n    \"expression\": \"0.0,100.0\",\n    \"message\": \"Score must be between 0 and 100\",\n    \"severity\": \"error\"\n  }\n]\n</code></pre>"},{"location":"features/data-validation/#3-severity-levels","title":"3. Severity Levels","text":"<ul> <li>Error: Critical issues that must be fixed</li> <li>Warning: Issues that should be investigated</li> </ul>"},{"location":"features/data-validation/#4-performance-optimization","title":"4. Performance Optimization","text":"<pre><code># Use sampling for large files\ndpa profile large_file.csv --sample 100000\ndpa validate large_file.csv --rules rules.json\n</code></pre>"},{"location":"features/data-validation/#integration-examples","title":"Integration Examples","text":""},{"location":"features/data-validation/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code># .github/workflows/validate.yml\nname: Data Validation\non: [push, pull_request]\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Validate Data\n        run: |\n          dpa validate data/production.csv --schema schema.json --rules rules.json\n</code></pre>"},{"location":"features/data-validation/#automated-monitoring","title":"Automated Monitoring","text":"<pre><code>import dpa_core\nimport smtplib\nfrom email.mime.text import MIMEText\n\ndef validate_and_alert():\n    try:\n        dpa_core.validate_py(\"daily_data.csv\", \"schema.json\", \"rules.json\")\n        print(\"Validation passed\")\n    except Exception as e:\n        # Send alert\n        send_alert(\"Data validation failed\", str(e))\n\ndef send_alert(subject, message):\n    # Email alert implementation\n    pass\n</code></pre>"},{"location":"features/data-validation/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Validate data in notebook\ntry:\n    dpa_core.validate_py(\"data.csv\", \"schema.json\", \"rules.json\")\n    print(\"\u2705 Data validation passed\")\nexcept Exception as e:\n    print(f\"\u274c Validation failed: {e}\")\n\n    # Load and analyze invalid data\n    invalid_df = pd.read_csv(\"invalid_rows.csv\")\n    print(f\"Found {len(invalid_df)} invalid rows\")\n</code></pre>"},{"location":"features/data-validation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/data-validation/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Schema Mismatch <pre><code># Check actual schema first\ndpa schema data.csv\n</code></pre></p> </li> <li> <p>Rule Syntax Errors <pre><code># Validate JSON syntax\npython -m json.tool rules.json\n</code></pre></p> </li> <li> <p>Performance Issues <pre><code># Use sampling for large files\ndpa validate large_file.csv --sample 100000\n</code></pre></p> </li> </ol>"},{"location":"features/data-validation/#error-messages","title":"Error Messages","text":"<ul> <li><code>\"Column not found\"</code>: Check column names in schema</li> <li><code>\"Invalid SQL expression\"</code>: Verify SQL syntax in rules</li> <li><code>\"File not found\"</code>: Check file paths and permissions</li> </ul>"},{"location":"features/data-validation/#advanced-features","title":"Advanced Features","text":""},{"location":"features/data-validation/#custom-validation-functions","title":"Custom Validation Functions","text":"<pre><code>import dpa_core\nimport json\n\ndef custom_validation(file_path, custom_rules):\n    \"\"\"Custom validation with additional logic\"\"\"\n\n    # Add custom rules\n    rules = [\n        {\n            \"name\": \"custom_check\",\n            \"column\": \"amount\",\n            \"rule_type\": \"sql\",\n            \"expression\": \"amount &gt; 1000\",\n            \"message\": \"Amount exceeds threshold\",\n            \"severity\": \"warning\"\n        }\n    ]\n\n    # Write rules to temporary file\n    with open(\"temp_rules.json\", \"w\") as f:\n        json.dump(rules, f)\n\n    try:\n        dpa_core.validate_py(file_path, rules_file=\"temp_rules.json\")\n        return True\n    except Exception as e:\n        print(f\"Validation failed: {e}\")\n        return False\n</code></pre>"},{"location":"features/data-validation/#batch-validation","title":"Batch Validation","text":"<pre><code>#!/bin/bash\n# Validate multiple files\nfor file in data/*.csv; do\n    echo \"Validating $file...\"\n    dpa validate \"$file\" --schema schema.json --rules rules.json -o \"validation_results/$(basename \"$file\" .csv)_errors.csv\"\ndone\n</code></pre> <p>This comprehensive validation system ensures data quality and consistency across your data processing workflows.</p>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>Learn how to configure DPA for optimal performance and customization.</p>"},{"location":"getting-started/configuration/#configuration-overview","title":"Configuration Overview","text":"<p>DPA can be configured through multiple methods:</p> <ul> <li>Environment Variables: Quick settings for current session</li> <li>Configuration Files: Persistent settings across sessions</li> <li>Command-line Options: Override settings for specific commands</li> <li>Python API Parameters: Programmatic configuration</li> </ul>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#core-configuration","title":"Core Configuration","text":"<pre><code># Set configuration file path\nexport DPA_CONFIG_PATH=/path/to/config.toml\n\n# Set log level (debug, info, warn, error)\nexport DPA_LOG_LEVEL=info\n\n# Set cache directory\nexport DPA_CACHE_DIR=~/.dpa/cache\n\n# Set maximum memory usage\nexport DPA_MAX_MEMORY=8GB\n\n# Set number of threads for parallel processing\nexport DPA_NUM_THREADS=4\n</code></pre>"},{"location":"getting-started/configuration/#performance-settings","title":"Performance Settings","text":"<pre><code># Enable/disable parallel processing\nexport DPA_PARALLEL=true\n\n# Set chunk size for large file processing\nexport DPA_CHUNK_SIZE=10000\n\n# Set buffer size for I/O operations\nexport DPA_BUFFER_SIZE=8192\n\n# Enable/disable memory mapping\nexport DPA_USE_MMAP=true\n</code></pre>"},{"location":"getting-started/configuration/#file-format-settings","title":"File Format Settings","text":"<pre><code># Default CSV delimiter\nexport DPA_CSV_DELIMITER=,\n\n# Default CSV quote character\nexport DPA_CSV_QUOTE=\"\n\n# Parquet compression level\nexport DPA_PARQUET_COMPRESSION=snappy\n\n# JSON pretty print\nexport DPA_JSON_PRETTY=true\n</code></pre>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#main-configuration-file","title":"Main Configuration File","text":"<p>Create <code>~/.dpa/config.toml</code>:</p> <pre><code>[general]\n# General settings\nlog_level = \"info\"\ncache_dir = \"~/.dpa/cache\"\nmax_memory = \"8GB\"\nnum_threads = 4\nparallel = true\n\n[profiling]\n# Profiling settings\nsample_size = 10000\ndetailed_stats = true\ninclude_memory_usage = true\ninclude_null_percentage = true\ninclude_unique_counts = true\ninclude_value_distributions = true\n\n[validation]\n# Validation settings\nstrict_mode = false\nmax_errors = 1000\ncontinue_on_error = true\noutput_invalid_rows = false\n\n[sampling]\n# Sampling settings\ndefault_method = \"random\"\ndefault_size = 1000\nstratified_min_group_size = 10\nrandom_seed = null\n\n[file_formats]\n# File format settings\ncsv_delimiter = \",\"\ncsv_quote = \"\\\"\"\ncsv_escape = \"\\\\\"\nparquet_compression = \"snappy\"\njson_pretty = true\n\n[performance]\n# Performance settings\nchunk_size = 10000\nbuffer_size = 8192\nuse_mmap = true\nlazy_evaluation = true\n\n[output]\n# Output settings\ndefault_format = \"csv\"\ninclude_header = true\nfloat_precision = 6\ndate_format = \"%Y-%m-%d\"\ndatetime_format = \"%Y-%m-%d %H:%M:%S\"\n</code></pre>"},{"location":"getting-started/configuration/#project-specific-configuration","title":"Project-Specific Configuration","text":"<p>Create <code>dpa.toml</code> in your project directory:</p> <pre><code>[project]\nname = \"my-data-project\"\nversion = \"1.0.0\"\n\n[data_sources]\nraw_data = \"data/raw/\"\nprocessed_data = \"data/processed/\"\noutput_data = \"data/output/\"\n\n[profiling]\n# Override global settings for this project\nsample_size = 50000\ndetailed_stats = false\n\n[validation]\n# Project-specific validation rules\nschema_file = \"schemas/data_schema.json\"\nrules_file = \"rules/validation_rules.json\"\n\n[sampling]\n# Project-specific sampling settings\nstratify_columns = [\"category\", \"region\"]\ntest_size = 0.2\n</code></pre>"},{"location":"getting-started/configuration/#command-line-configuration","title":"Command-Line Configuration","text":""},{"location":"getting-started/configuration/#global-options","title":"Global Options","text":"<pre><code># Set configuration file\n./target/release/dpa --config /path/to/config.toml profile data.csv\n\n# Override log level\n./target/release/dpa --log-level debug profile data.csv\n\n# Set output format\n./target/release/dpa --output-format parquet profile data.csv\n\n# Set number of threads\n./target/release/dpa --threads 8 profile data.csv\n</code></pre>"},{"location":"getting-started/configuration/#command-specific-options","title":"Command-Specific Options","text":"<pre><code># Profiling with custom settings\n./target/release/dpa profile data.csv \\\n  --sample-size 50000 \\\n  --detailed \\\n  --include-memory\n\n# Validation with custom settings\n./target/release/dpa validate data.csv \\\n  --schema schema.json \\\n  --rules rules.json \\\n  --max-errors 100 \\\n  --output-invalid\n\n# Sampling with custom settings\n./target/release/dpa sample data.csv output.csv \\\n  --method stratified \\\n  --size 1000 \\\n  --stratify category \\\n  --seed 42\n</code></pre>"},{"location":"getting-started/configuration/#python-api-configuration","title":"Python API Configuration","text":""},{"location":"getting-started/configuration/#global-configuration","title":"Global Configuration","text":"<pre><code>import dpa_core\nimport os\n\n# Set environment variables\nos.environ['DPA_LOG_LEVEL'] = 'debug'\nos.environ['DPA_MAX_MEMORY'] = '4GB'\n\n# Configure through Python\ndpa_core.set_config({\n    'log_level': 'info',\n    'max_memory': '4GB',\n    'num_threads': 4\n})\n</code></pre>"},{"location":"getting-started/configuration/#function-specific-configuration","title":"Function-Specific Configuration","text":"<pre><code>import dpa_core\n\n# Profile with custom settings\nstats = dpa_core.profile_py(\n    \"data.csv\",\n    sample_size=50000,\n    detailed=True,\n    include_memory=True\n)\n\n# Sample with custom settings\ndpa_core.sample_py(\n    \"data.csv\",\n    \"output.csv\",\n    size=1000,\n    method=\"stratified\",\n    stratify=\"category\",\n    seed=42\n)\n\n# Validate with custom settings\ndpa_core.validate_py(\n    \"data.csv\",\n    schema=\"schema.json\",\n    rules=\"rules.json\"\n)\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/configuration/#memory-management","title":"Memory Management","text":"<pre><code>[memory]\n# Memory management settings\nmax_memory = \"8GB\"\nmemory_fraction = 0.8\nspill_to_disk = true\nspill_directory = \"/tmp/dpa_spill\"\ncleanup_on_exit = true\n</code></pre>"},{"location":"getting-started/configuration/#caching-configuration","title":"Caching Configuration","text":"<pre><code>[cache]\n# Caching settings\nenabled = true\ncache_dir = \"~/.dpa/cache\"\nmax_cache_size = \"2GB\"\ncache_ttl = 3600  # seconds\ncleanup_interval = 86400  # seconds\n</code></pre>"},{"location":"getting-started/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>[logging]\n# Logging settings\nlevel = \"info\"\nformat = \"{time} {level} {message}\"\nfile = \"~/.dpa/logs/dpa.log\"\nmax_file_size = \"10MB\"\nmax_files = 5\n</code></pre>"},{"location":"getting-started/configuration/#performance-tuning","title":"Performance Tuning","text":"<pre><code>[performance]\n# Performance tuning\nchunk_size = 10000\nbuffer_size = 8192\nuse_mmap = true\nlazy_evaluation = true\nparallel_threshold = 10000\nrayon_threads = 4\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>DPA follows this precedence order (highest to lowest):</p> <ol> <li>Command-line options (highest priority)</li> <li>Function parameters (Python API)</li> <li>Project configuration (<code>dpa.toml</code>)</li> <li>User configuration (<code>~/.dpa/config.toml</code>)</li> <li>Environment variables</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":""},{"location":"getting-started/configuration/#validate-configuration-file","title":"Validate Configuration File","text":"<pre><code># Validate configuration syntax\n./target/release/dpa config validate config.toml\n\n# Check configuration\n./target/release/dpa config check\n</code></pre>"},{"location":"getting-started/configuration/#python-configuration-validation","title":"Python Configuration Validation","text":"<pre><code>import dpa_core\n\n# Validate configuration\nconfig = {\n    'log_level': 'info',\n    'max_memory': '8GB',\n    'num_threads': 4\n}\n\ntry:\n    dpa_core.validate_config(config)\n    print(\"\u2705 Configuration is valid\")\nexcept Exception as e:\n    print(f\"\u274c Configuration error: {e}\")\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/configuration/#1-use-project-specific-configurations","title":"1. Use Project-Specific Configurations","text":"<pre><code># Create project configuration\ncat &gt; dpa.toml &lt;&lt; EOF\n[project]\nname = \"my-project\"\n\n[profiling]\nsample_size = 50000\n\n[validation]\nschema_file = \"schemas/data_schema.json\"\nEOF\n</code></pre>"},{"location":"getting-started/configuration/#2-environment-specific-settings","title":"2. Environment-Specific Settings","text":"<pre><code># Development environment\nexport DPA_LOG_LEVEL=debug\nexport DPA_MAX_MEMORY=4GB\n\n# Production environment\nexport DPA_LOG_LEVEL=warn\nexport DPA_MAX_MEMORY=16GB\n</code></pre>"},{"location":"getting-started/configuration/#3-monitor-performance","title":"3. Monitor Performance","text":"<pre><code># Enable performance monitoring\nexport DPA_PROFILE_PERFORMANCE=true\nexport DPA_PERFORMANCE_LOG=performance.log\n</code></pre>"},{"location":"getting-started/configuration/#4-secure-configuration","title":"4. Secure Configuration","text":"<pre><code># Use secure file permissions\nchmod 600 ~/.dpa/config.toml\n\n# Don't commit sensitive configuration\necho \"~/.dpa/config.toml\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#common-configuration-issues","title":"Common Configuration Issues","text":"<p>1. Configuration File Not Found <pre><code># Check configuration path\necho $DPA_CONFIG_PATH\nls -la ~/.dpa/config.toml\n</code></pre></p> <p>2. Invalid Configuration Syntax <pre><code># Validate TOML syntax\n./target/release/dpa config validate config.toml\n</code></pre></p> <p>3. Memory Issues <pre><code># Reduce memory usage\nexport DPA_MAX_MEMORY=4GB\nexport DPA_CHUNK_SIZE=5000\n</code></pre></p> <p>4. Performance Issues <pre><code># Optimize performance\nexport DPA_NUM_THREADS=8\nexport DPA_PARALLEL=true\nexport DPA_USE_MMAP=true\n</code></pre></p>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udd27 Customize Settings: Adjust configuration for your needs</li> <li>\ud83d\udcca Monitor Performance: Use performance settings to optimize</li> <li>\ud83d\udee1\ufe0f Security: Implement secure configuration practices</li> <li>\ud83d\udcda Examples: See configuration examples in the Examples section</li> </ul> <p>Configuration complete! Your DPA setup is optimized and ready. \u2699\ufe0f</p>"},{"location":"getting-started/first-steps/","title":"First Steps with DPA","text":"<p>Welcome to your first steps with Data Processing Accelerator (DPA)! This guide will walk you through your first data processing tasks.</p>"},{"location":"getting-started/first-steps/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 DPA installed (see Installation Guide)</li> <li>\u2705 A sample dataset to work with</li> <li>\u2705 Basic familiarity with command line or Python</li> </ul>"},{"location":"getting-started/first-steps/#getting-your-first-dataset","title":"Getting Your First Dataset","text":""},{"location":"getting-started/first-steps/#option-1-use-sample-data","title":"Option 1: Use Sample Data","text":"<p>DPA comes with sample datasets for testing:</p> <pre><code># Check available sample data\nls data/\nls test_data/\n</code></pre>"},{"location":"getting-started/first-steps/#option-2-create-your-own","title":"Option 2: Create Your Own","text":"<p>Create a simple CSV file for testing:</p> <pre><code># Create a sample CSV file\ncat &gt; sample_data.csv &lt;&lt; EOF\nid,name,age,city,salary\n1,Alice,25,New York,50000\n2,Bob,30,San Francisco,75000\n3,Charlie,35,Chicago,60000\n4,Diana,28,Boston,65000\n5,Eve,32,Seattle,70000\nEOF\n</code></pre>"},{"location":"getting-started/first-steps/#your-first-dpa-operations","title":"Your First DPA Operations","text":""},{"location":"getting-started/first-steps/#step-1-explore-your-data","title":"Step 1: Explore Your Data","text":"<p>Start by understanding your data structure:</p> <pre><code># View the first few rows\n./target/release/dpa head sample_data.csv -n 5\n\n# Get schema information\n./target/release/dpa schema sample_data.csv\n</code></pre>"},{"location":"getting-started/first-steps/#step-2-profile-your-data","title":"Step 2: Profile Your Data","text":"<p>Get comprehensive insights about your data:</p> <pre><code># Basic profiling\n./target/release/dpa profile sample_data.csv\n\n# Detailed profiling with statistics\n./target/release/dpa profile sample_data.csv --detailed\n</code></pre>"},{"location":"getting-started/first-steps/#step-3-validate-your-data","title":"Step 3: Validate Your Data","text":"<p>Check data quality and consistency:</p> <pre><code># Basic validation\n./target/release/dpa validate sample_data.csv\n\n# Create a simple schema for validation\ncat &gt; schema.json &lt;&lt; EOF\n{\n  \"columns\": {\n    \"id\": {\"type\": \"integer\", \"nullable\": false},\n    \"name\": {\"type\": \"string\", \"nullable\": false},\n    \"age\": {\"type\": \"integer\", \"min\": 18, \"max\": 100},\n    \"city\": {\"type\": \"string\", \"nullable\": false},\n    \"salary\": {\"type\": \"float\", \"min\": 0}\n  }\n}\nEOF\n\n# Validate against schema\n./target/release/dpa validate sample_data.csv --schema schema.json\n</code></pre>"},{"location":"getting-started/first-steps/#step-4-sample-your-data","title":"Step 4: Sample Your Data","text":"<p>Create smaller datasets for testing:</p> <pre><code># Random sampling\n./target/release/dpa sample sample_data.csv sample_random.csv --method random --size 3\n\n# Stratified sampling (if you have categorical data)\n./target/release/dpa sample sample_data.csv sample_stratified.csv --method stratified --stratify city\n</code></pre>"},{"location":"getting-started/first-steps/#step-5-transform-your-data","title":"Step 5: Transform Your Data","text":"<p>Convert between file formats:</p> <pre><code># Convert CSV to Parquet\n./target/release/dpa convert sample_data.csv sample_data.parquet\n\n# Convert Parquet back to CSV\n./target/release/dpa convert sample_data.parquet sample_data_new.csv\n</code></pre>"},{"location":"getting-started/first-steps/#using-python-api","title":"Using Python API","text":""},{"location":"getting-started/first-steps/#basic-python-operations","title":"Basic Python Operations","text":"<pre><code>import dpa_core\nimport pandas as pd\n\n# Profile data\nstats = dpa_core.profile_py(\"sample_data.csv\")\nprint(\"Data Statistics:\")\nfor key, value in stats.items():\n    print(f\"  {key}: {value}\")\n\n# Sample data\ndpa_core.sample_py(\"sample_data.csv\", \"python_sample.csv\", size=3, method=\"random\")\n\n# Validate data\ntry:\n    dpa_core.validate_py(\"sample_data.csv\")\n    print(\"\u2705 Data validation passed!\")\nexcept Exception as e:\n    print(f\"\u274c Validation failed: {e}\")\n\n# Filter data\ndpa_core.filter_py(\"sample_data.csv\", \"age &gt; 30\", output=\"filtered_data.csv\")\n\n# Select columns\ndpa_core.select_py(\"sample_data.csv\", [\"name\", \"age\", \"salary\"], output=\"selected_data.csv\")\n</code></pre>"},{"location":"getting-started/first-steps/#working-with-results","title":"Working with Results","text":"<pre><code># Read the results back into Python\nimport pandas as pd\n\n# Read original data\ndf = pd.read_csv(\"sample_data.csv\")\nprint(\"Original data shape:\", df.shape)\n\n# Read filtered data\nfiltered_df = pd.read_csv(\"filtered_data.csv\")\nprint(\"Filtered data shape:\", filtered_df.shape)\n\n# Read sampled data\nsampled_df = pd.read_csv(\"python_sample.csv\")\nprint(\"Sampled data shape:\", sampled_df.shape)\n</code></pre>"},{"location":"getting-started/first-steps/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/first-steps/#data-exploration-workflow","title":"Data Exploration Workflow","text":"<pre><code># 1. Quick overview\n./target/release/dpa head your_data.csv -n 10\n\n# 2. Schema inspection\n./target/release/dpa schema your_data.csv\n\n# 3. Detailed profiling\n./target/release/dpa profile your_data.csv --detailed\n\n# 4. Data validation\n./target/release/dpa validate your_data.csv\n</code></pre>"},{"location":"getting-started/first-steps/#data-preparation-workflow","title":"Data Preparation Workflow","text":"<pre><code># 1. Sample for development\n./target/release/dpa sample your_data.csv dev_sample.csv --method random --size 1000\n\n# 2. Validate sample\n./target/release/dpa validate dev_sample.csv\n\n# 3. Convert to efficient format\n./target/release/dpa convert dev_sample.csv dev_sample.parquet\n\n# 4. Create train/test split\n./target/release/dpa split dev_sample.csv train.csv test.csv --test-size 0.2\n</code></pre>"},{"location":"getting-started/first-steps/#quality-assurance-workflow","title":"Quality Assurance Workflow","text":"<pre><code>import dpa_core\n\n# 1. Profile data\nstats = dpa_core.profile_py(\"your_data.csv\")\n\n# 2. Check for issues\nif float(stats.get(\"null_percentage\", 0)) &gt; 0.1:\n    print(\"\u26a0\ufe0f  High null percentage detected\")\n\n# 3. Validate data\ntry:\n    dpa_core.validate_py(\"your_data.csv\")\n    print(\"\u2705 Data quality check passed\")\nexcept Exception as e:\n    print(f\"\u274c Quality issues found: {e}\")\n\n# 4. Create clean sample\ndpa_core.sample_py(\"your_data.csv\", \"clean_sample.csv\", size=1000, method=\"random\")\n</code></pre>"},{"location":"getting-started/first-steps/#tips-for-beginners","title":"Tips for Beginners","text":""},{"location":"getting-started/first-steps/#1-start-small","title":"1. Start Small","text":"<ul> <li>Begin with small datasets to understand the tools</li> <li>Use the sample data provided with DPA</li> <li>Test commands on subsets before processing large files</li> </ul>"},{"location":"getting-started/first-steps/#2-use-help-commands","title":"2. Use Help Commands","text":"<pre><code># Get help for any command\n./target/release/dpa --help\n./target/release/dpa profile --help\n./target/release/dpa sample --help\n</code></pre>"},{"location":"getting-started/first-steps/#3-check-file-formats","title":"3. Check File Formats","text":"<ul> <li>DPA supports CSV, Parquet, and JSON</li> <li>Use Parquet for large datasets (better compression)</li> <li>Use CSV for compatibility with other tools</li> </ul>"},{"location":"getting-started/first-steps/#4-monitor-performance","title":"4. Monitor Performance","text":"<ul> <li>Start with small sample sizes</li> <li>Use <code>--detailed</code> flag sparingly on large datasets</li> <li>Monitor memory usage with large files</li> </ul>"},{"location":"getting-started/first-steps/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first steps:</p> <ol> <li>\ud83c\udfaf Explore Features: Learn about Data Profiling, Validation, and Sampling</li> <li>\ud83d\udcda Try Examples: Work through Basic Usage examples</li> <li>\ud83d\udd27 Learn Configuration: Understand Configuration Options</li> <li>\ud83d\ude80 Scale Up: Move to larger datasets and more complex workflows</li> </ol>"},{"location":"getting-started/first-steps/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation: Browse the full documentation</li> <li>\ud83d\udca1 Examples: Check the examples directory</li> <li>\ud83d\udc1b Issues: Report problems on GitHub</li> <li>\ud83d\udcac Community: Join discussions</li> </ul> <p>Congratulations! You've taken your first steps with DPA. \ud83c\udf89</p> <p>Ready to accelerate your data processing journey!</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide covers all installation methods for Data Processing Accelerator (DPA).</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Operating System: Linux, macOS, or Windows</li> <li>Rust: 1.70 or later</li> <li>Python: 3.8 or later</li> <li>Memory: 4GB RAM (8GB recommended for large datasets)</li> <li>Storage: 2GB free space</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>CPU: Multi-core processor (4+ cores)</li> <li>Memory: 16GB RAM or more</li> <li>Storage: SSD with 10GB+ free space</li> <li>Network: Internet connection for package downloads</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-from-source-recommended","title":"Method 1: From Source (Recommended)","text":""},{"location":"getting-started/installation/#step-1-install-prerequisites","title":"Step 1: Install Prerequisites","text":"<p>Install Rust: <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\nrustup default stable\n</code></pre></p> <p>Install Python: <pre><code># On Ubuntu/Debian\nsudo apt update\nsudo apt install python3 python3-pip python3-venv\n\n# On macOS with Homebrew\nbrew install python3\n\n# On Windows\n# Download from https://python.org\n</code></pre></p>"},{"location":"getting-started/installation/#step-2-clone-and-build","title":"Step 2: Clone and Build","text":"<pre><code># Clone the repository\ngit clone https://github.com/your-username/dpa-full-regenerated\ncd dpa-full-regenerated\n\n# Build the Rust binary\ncargo build --release\n\n# Install Python dependencies\npip install maturin\n\n# Build and install Python bindings\nmaturin develop\n\n# Install Python CLI\ncd python\npip install .\n</code></pre>"},{"location":"getting-started/installation/#step-3-verify-installation","title":"Step 3: Verify Installation","text":"<pre><code># Test CLI\n./target/release/dpa --help\n\n# Test Python API\npython -c \"import dpa_core; print('DPA installed successfully!')\"\n</code></pre>"},{"location":"getting-started/installation/#method-2-using-pip-python-only","title":"Method 2: Using pip (Python Only)","text":"<pre><code># Install from PyPI (when available)\npip install dpa\n\n# Or install from GitHub\npip install git+https://github.com/your-username/dpa-full-regenerated.git\n</code></pre>"},{"location":"getting-started/installation/#method-3-using-cargo-rust-only","title":"Method 3: Using Cargo (Rust Only)","text":"<pre><code># Install from crates.io (when available)\ncargo install dpa\n\n# Or install from GitHub\ncargo install --git https://github.com/your-username/dpa-full-regenerated.git\n</code></pre>"},{"location":"getting-started/installation/#platform-specific-instructions","title":"Platform-Specific Instructions","text":""},{"location":"getting-started/installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code># Install system dependencies\nsudo apt update\nsudo apt install build-essential pkg-config libssl-dev\n\n# Install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource ~/.cargo/env\n\n# Install Python\nsudo apt install python3 python3-pip python3-venv\n\n# Follow the source installation steps above\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<pre><code># Install Homebrew if not installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install dependencies\nbrew install rust python3\n\n# Follow the source installation steps above\n</code></pre>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ol> <li>Install Rust:</li> <li>Download rustup-init.exe from https://rustup.rs</li> <li> <p>Run the installer and follow the prompts</p> </li> <li> <p>Install Python:</p> </li> <li>Download from https://python.org</li> <li> <p>Ensure \"Add Python to PATH\" is checked</p> </li> <li> <p>Install Visual Studio Build Tools:</p> </li> <li>Download from Microsoft Visual Studio</li> <li> <p>Install C++ build tools</p> </li> <li> <p>Follow the source installation steps above</p> </li> </ol>"},{"location":"getting-started/installation/#virtual-environment-setup","title":"Virtual Environment Setup","text":"<pre><code># Create virtual environment\npython3 -m venv dpa-env\n\n# Activate virtual environment\n# On Linux/macOS:\nsource dpa-env/bin/activate\n\n# On Windows:\ndpa-env\\Scripts\\activate\n\n# Install DPA in virtual environment\npip install maturin\nmaturin develop\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":""},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<pre><code># Set DPA configuration\nexport DPA_CONFIG_PATH=/path/to/config\nexport DPA_LOG_LEVEL=info\nexport DPA_CACHE_DIR=/path/to/cache\n</code></pre>"},{"location":"getting-started/installation/#configuration-file","title":"Configuration File","text":"<p>Create <code>~/.dpa/config.toml</code>:</p> <pre><code>[general]\nlog_level = \"info\"\ncache_dir = \"~/.dpa/cache\"\nmax_memory = \"8GB\"\n\n[profiling]\nsample_size = 10000\ndetailed_stats = true\n\n[validation]\nstrict_mode = false\nmax_errors = 1000\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>1. Rust Build Errors <pre><code># Update Rust\nrustup update\n\n# Clean and rebuild\ncargo clean\ncargo build --release\n</code></pre></p> <p>2. Python Import Errors <pre><code># Reinstall Python bindings\nmaturin develop --force-reinstall\n\n# Check Python version\npython --version\n</code></pre></p> <p>3. Memory Issues <pre><code># Increase system memory or reduce dataset size\nexport DPA_MAX_MEMORY=4GB\n</code></pre></p> <p>4. Permission Errors <pre><code># Fix permissions\nchmod +x target/release/dpa\n</code></pre></p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation: Check this documentation</li> <li>\ud83d\udc1b GitHub Issues: Report bugs on GitHub</li> <li>\ud83d\udcac Discussions: Join community discussions</li> <li>\ud83d\udce7 Email: Contact support team</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation:</p> <ol> <li>\ud83d\ude80 Try the Quick Start Guide</li> <li>\ud83d\udcd6 Read the User Guide</li> <li>\ud83c\udfaf Explore Examples</li> <li>\ud83d\udd27 Learn about Configuration</li> </ol> <p>Installation complete! Ready to accelerate your data processing. \u26a1</p>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Welcome to Data Processing Accelerator (DPA)! This guide will get you up and running in minutes.</p>"},{"location":"getting-started/quick-start/#what-is-dpa","title":"What is DPA?","text":"<p>DPA is a high-performance data processing tool built with Rust and Polars, featuring:</p> <ul> <li>Lightning-fast performance with Rust and Polars</li> <li>Python API for seamless integration</li> <li>Command-line interface for quick operations</li> <li>Advanced data profiling and validation</li> <li>Multiple file formats (CSV, Parquet, JSON)</li> <li>Smart sampling and data splitting</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust (1.70 or later)</li> <li>Python (3.8 or later)</li> <li>pip for Python package management</li> </ul>"},{"location":"getting-started/quick-start/#quick-installation","title":"Quick Installation","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/your-username/dpa-full-regenerated\ncd dpa-full-regenerated\n</code></pre></p> </li> <li> <p>Build the Rust binary:    <pre><code>cargo build --release\n</code></pre></p> </li> <li> <p>Install Python bindings:    <pre><code>pip install maturin\nmaturin develop\n</code></pre></p> </li> <li> <p>Install Python CLI:    <pre><code>cd python\npip install .\n</code></pre></p> </li> </ol>"},{"location":"getting-started/quick-start/#your-first-dpa-operations","title":"Your First DPA Operations","text":""},{"location":"getting-started/quick-start/#1-data-profiling","title":"1. Data Profiling","text":"<p>Profile your data to understand its structure and quality:</p> <pre><code># Basic profiling\n./target/release/dpa profile data/transactions_small.csv\n\n# Detailed profiling with statistics\n./target/release/dpa profile data/transactions_small.csv --detailed\n</code></pre>"},{"location":"getting-started/quick-start/#2-data-validation","title":"2. Data Validation","text":"<p>Validate your data against schemas and rules:</p> <pre><code># Basic validation\n./target/release/dpa validate data/transactions_small.csv\n\n# Schema validation\n./target/release/dpa validate data/transactions_small.csv --schema examples/schema.json\n</code></pre>"},{"location":"getting-started/quick-start/#3-data-sampling","title":"3. Data Sampling","text":"<p>Sample your data for analysis or testing:</p> <pre><code># Random sampling\n./target/release/dpa sample data/transactions_small.csv output.csv --method random --size 1000\n\n# Stratified sampling\n./target/release/dpa sample data/transactions_small.csv output.csv --method stratified --stratify category\n</code></pre>"},{"location":"getting-started/quick-start/#4-using-python-api","title":"4. Using Python API","text":"<pre><code>import dpa_core\n\n# Profile data\nstats = dpa_core.profile_py(\"data/transactions_small.csv\")\nprint(stats)\n\n# Sample data\ndpa_core.sample_py(\"data/transactions_small.csv\", \"sample.csv\", size=1000, method=\"random\")\n\n# Validate data\ndpa_core.validate_py(\"data/transactions_small.csv\")\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Installation Guide for detailed setup instructions</li> <li>Explore Features to learn about all capabilities</li> <li>Check out Examples for practical use cases</li> <li>See Configuration for advanced setup</li> </ul>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse the full documentation</li> <li>Issues: Report bugs on GitHub</li> <li>Discussions: Join community discussions</li> <li>Support: Contact the development team</li> </ul> <p>Ready to accelerate your data processing?</p>"},{"location":"user-guide/overview/","title":"DPA Overview","text":"<p>Data Processing Accelerator (DPA) is a high-performance data processing tool designed to handle large-scale data operations with speed and efficiency.</p>"},{"location":"user-guide/overview/#what-is-dpa","title":"What is DPA?","text":"<p>DPA is a modern data processing solution that combines:</p> <ul> <li>Rust Performance: Built with Rust for maximum speed and memory efficiency</li> <li>Polars Integration: Leverages Polars for fast DataFrame operations</li> <li>Python API: Seamless Python integration with PyO3 bindings</li> <li>CLI Interface: Command-line tools for quick data operations</li> <li>Multiple Formats: Support for CSV, Parquet, and JSON files</li> <li>Smart Features: Advanced profiling, validation, and sampling</li> </ul>"},{"location":"user-guide/overview/#key-features","title":"Key Features","text":""},{"location":"user-guide/overview/#high-performance","title":"High Performance","text":"<ul> <li>Rust-powered: Near-native performance with memory safety</li> <li>Polars backend: Optimized DataFrame operations</li> <li>Parallel processing: Multi-threaded operations for large datasets</li> <li>Memory efficient: Smart memory management and streaming</li> </ul>"},{"location":"user-guide/overview/#data-profiling","title":"Data Profiling","text":"<ul> <li>Comprehensive statistics: Min, max, mean, std, percentiles</li> <li>Data quality metrics: Null percentages, unique counts, memory usage</li> <li>Value distributions: Most common values, average string lengths</li> <li>Outlier detection: Statistical outliers and anomalies</li> <li>Detailed reports: Rich formatted output with visualizations</li> </ul>"},{"location":"user-guide/overview/#data-validation","title":"Data Validation","text":"<ul> <li>Schema validation: Verify column types and structure</li> <li>Data type detection: Identify mixed types and inconsistencies</li> <li>Range validation: Check numeric ranges and detect outliers</li> <li>Custom rules: SQL-based validation rules with error/warning levels</li> <li>Quality reporting: Detailed validation reports with counts</li> </ul>"},{"location":"user-guide/overview/#smart-sampling","title":"Smart Sampling","text":"<ul> <li>Multiple methods: Random, stratified, head, tail sampling</li> <li>Stratified sampling: Maintain distribution of categorical columns</li> <li>Train/test splits: Machine learning ready data splitting</li> <li>Reproducible results: Seeded random sampling for consistency</li> <li>Performance optimized: Efficient sampling for large datasets</li> </ul>"},{"location":"user-guide/overview/#file-operations","title":"File Operations","text":"<ul> <li>Format conversion: Convert between CSV, Parquet, and JSON</li> <li>Column selection: Select specific columns from datasets</li> <li>Data filtering: SQL-like expressions for data filtering</li> <li>Aggregations: Groupby operations with multiple aggregation functions</li> <li>Joins: Merge datasets with various join strategies</li> </ul>"},{"location":"user-guide/overview/#architecture","title":"Architecture","text":""},{"location":"user-guide/overview/#core-components","title":"Core Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Interface \u2502    \u2502  Python API     \u2502    \u2502   Rust Engine   \u2502\n\u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 \u2022 Commands      \u2502    \u2502 \u2022 PyO3 Bindings \u2502    \u2502 \u2022 Polars Core   \u2502\n\u2502 \u2022 Arguments     \u2502    \u2502 \u2022 Functions     \u2502    \u2502 \u2022 Data Processing\u2502\n\u2502 \u2022 Help System   \u2502    \u2502 \u2022 Error Handling\u2502    \u2502 \u2022 Memory Mgmt   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   File I/O      \u2502\n                    \u2502                 \u2502\n                    \u2502 \u2022 CSV Reader    \u2502\n                    \u2502 \u2022 Parquet I/O   \u2502\n                    \u2502 \u2022 JSON Support  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/overview/#technology-stack","title":"Technology Stack","text":"<ul> <li>Rust: Core language for performance and safety</li> <li>Polars: Fast DataFrame library for data operations</li> <li>PyO3: Python bindings for seamless integration</li> <li>Clap: Command-line argument parsing</li> <li>Serde: Serialization/deserialization</li> <li>Rayon: Parallel processing</li> <li>Tokio: Async runtime (when needed)</li> </ul>"},{"location":"user-guide/overview/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/overview/#data-science-analytics","title":"Data Science &amp; Analytics","text":"<ul> <li>Exploratory Data Analysis: Quick profiling and validation</li> <li>Data Quality Assessment: Identify issues and inconsistencies</li> <li>Feature Engineering: Prepare data for machine learning</li> <li>Model Validation: Create train/test splits and samples</li> </ul>"},{"location":"user-guide/overview/#data-engineering","title":"Data Engineering","text":"<ul> <li>ETL Pipelines: Transform and load data efficiently</li> <li>Data Validation: Ensure data quality in pipelines</li> <li>Format Conversion: Convert between different file formats</li> <li>Performance Optimization: Process large datasets quickly</li> </ul>"},{"location":"user-guide/overview/#business-intelligence","title":"Business Intelligence","text":"<ul> <li>Data Profiling: Understand data structure and quality</li> <li>Reporting: Generate data quality reports</li> <li>Data Sampling: Create representative samples for analysis</li> <li>Validation: Ensure business rules are met</li> </ul>"},{"location":"user-guide/overview/#research-development","title":"Research &amp; Development","text":"<ul> <li>Prototyping: Quick data exploration and validation</li> <li>Testing: Create test datasets and validate results</li> <li>Performance Testing: Benchmark data processing operations</li> <li>Research: Analyze large datasets efficiently</li> </ul>"},{"location":"user-guide/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"user-guide/overview/#speed-comparison","title":"Speed Comparison","text":"Operation DPA Pandas PyArrow Dask CSV Read (1GB) 2.1s 8.5s 3.2s 4.8s Parquet Read (1GB) 1.8s 2.9s 2.1s 3.1s Groupby (1M rows) 0.8s 2.3s 1.5s 1.9s Memory Usage 512MB 1.2GB 890MB 1.1GB"},{"location":"user-guide/overview/#scalability","title":"Scalability","text":"<ul> <li>Small datasets (&lt; 1GB): Real-time processing</li> <li>Medium datasets (1-10GB): Efficient memory usage</li> <li>Large datasets (10-100GB): Streaming and chunking</li> <li>Very large datasets (&gt; 100GB): Distributed processing ready</li> </ul>"},{"location":"user-guide/overview/#getting-started","title":"Getting Started","text":""},{"location":"user-guide/overview/#quick-installation","title":"Quick Installation","text":"<pre><code># Clone and build\ngit clone https://github.com/your-username/dpa-full-regenerated\ncd dpa-full-regenerated\ncargo build --release\n\n# Install Python bindings\npip install maturin\nmaturin develop\n</code></pre>"},{"location":"user-guide/overview/#first-operations","title":"First Operations","text":"<pre><code># Profile your data\n./target/release/dpa profile data.csv\n\n# Sample your data\n./target/release/dpa sample data.csv sample.csv --size 1000\n\n# Validate your data\n./target/release/dpa validate data.csv\n</code></pre>"},{"location":"user-guide/overview/#python-api","title":"Python API","text":"<pre><code>import dpa_core\n\n# Profile data\nstats = dpa_core.profile_py(\"data.csv\")\nprint(stats)\n\n# Sample data\ndpa_core.sample_py(\"data.csv\", \"sample.csv\", size=1000)\n\n# Validate data\ndpa_core.validate_py(\"data.csv\")\n</code></pre>"},{"location":"user-guide/overview/#comparison-with-other-tools","title":"Comparison with Other Tools","text":""},{"location":"user-guide/overview/#vs-pandas","title":"vs Pandas","text":"<ul> <li>Speed: 3-5x faster for most operations</li> <li>Memory: 50-70% less memory usage</li> <li>Scalability: Better handling of large datasets</li> <li>Features: Built-in profiling and validation</li> </ul>"},{"location":"user-guide/overview/#vs-pyarrow","title":"vs PyArrow","text":"<ul> <li>Ease of use: More intuitive API</li> <li>Features: Advanced profiling and sampling</li> <li>Integration: Better Python ecosystem integration</li> <li>Performance: Comparable speed with more features</li> </ul>"},{"location":"user-guide/overview/#vs-dask","title":"vs Dask","text":"<ul> <li>Simplicity: Easier to use for single-machine operations</li> <li>Performance: Faster for in-memory operations</li> <li>Memory: More efficient memory usage</li> <li>Features: Built-in data quality tools</li> </ul>"},{"location":"user-guide/overview/#roadmap","title":"Roadmap","text":""},{"location":"user-guide/overview/#current-version-v021","title":"Current Version (v0.2.1)","text":"<ul> <li>\u2705 Core data processing operations</li> <li>\u2705 Python API with PyO3</li> <li>\u2705 CLI interface</li> <li>\u2705 Data profiling and validation</li> <li>\u2705 Sampling and splitting</li> <li>\u2705 Multiple file format support</li> </ul>"},{"location":"user-guide/overview/#upcoming-features","title":"Upcoming Features","text":"<ul> <li>\ud83d\udd04 Distributed Processing: Multi-node support</li> <li>\ud83d\udd04 Streaming: Real-time data processing</li> <li>\ud83d\udd04 Machine Learning: Built-in ML algorithms</li> <li>\ud83d\udd04 Visualization: Data visualization tools</li> <li>\ud83d\udd04 Cloud Integration: AWS, GCP, Azure support</li> <li>\ud83d\udd04 Web Interface: Web-based UI</li> </ul>"},{"location":"user-guide/overview/#community-support","title":"Community &amp; Support","text":""},{"location":"user-guide/overview/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation: Comprehensive guides and examples</li> <li>\ud83d\udc1b GitHub Issues: Report bugs and request features</li> <li>\ud83d\udcac Discussions: Community discussions and Q&amp;A</li> <li>\ud83d\udce7 Email Support: Direct support for enterprise users</li> </ul>"},{"location":"user-guide/overview/#contributing","title":"Contributing","text":"<ul> <li>\ud83d\udd27 Code Contributions: Pull requests welcome</li> <li>\ud83d\udcda Documentation: Help improve docs</li> <li>\ud83e\uddea Testing: Report bugs and test features</li> <li>\ud83d\udca1 Ideas: Suggest new features and improvements</li> </ul>"},{"location":"user-guide/overview/#resources","title":"Resources","text":"<ul> <li>\ud83d\udcd6 User Guide: Detailed usage instructions</li> <li>\ud83c\udfaf Examples: Practical examples and use cases</li> <li>\ud83d\udd27 API Reference: Complete API documentation</li> <li>\ud83d\ude80 Tutorials: Step-by-step tutorials</li> </ul> <p>Ready to accelerate your data processing? \ud83d\ude80</p> <p>Start with the Quick Start Guide or explore the Features section to learn more about DPA's capabilities.</p>"}]}